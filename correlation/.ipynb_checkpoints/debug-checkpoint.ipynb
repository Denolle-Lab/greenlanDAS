{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a2ad8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflow_old (generic function with 6 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SeisNoise, PyPlot, CUDA, Glob, HDF5, Combinatorics, Random, Statistics, ImageFiltering, FFTW, JLD2, Dates\n",
    "import SeisNoise: NoiseData\n",
    "import SeisIO: read_nodal, NodalData, InstrumentPosition, InstrumentResponse, show_str, show_t, show_x, show_os\n",
    "import FFTW: rfft, irfft\n",
    "import DSP: hilbert\n",
    "import Images: findlocalmaxima\n",
    "import Base:show, size, summary\n",
    "import PyCall\n",
    "import\n",
    "SeisDvv\n",
    "import Dates\n",
    "import FiniteDifferences\n",
    "import CSV\n",
    "using NetCDF\n",
    "using DataFrames\n",
    "include(\"Types.jl\")\n",
    "include(\"Nodal.jl\")\n",
    "include(\"Misc.jl\")\n",
    "include(\"Workflow.jl\")\n",
    "include(\"Workflow_old.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec05d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflow_new_files (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function workflow_old_files(files,output_times,f_desired,samples_per_file=[])\n",
    "    \n",
    "    # read the first file and collect metadata\n",
    "    N = read_nodal(\"segy\", files[1])\n",
    "    datetime = get_datetime(N)\n",
    "    if isempty(samples_per_file) == true\n",
    "        samples_per_file = N.info[\"orignx\"]\n",
    "    end\n",
    "    seconds_per_file = samples_per_file/N.fs[1]\n",
    "    num_files = length(files)\n",
    "    \n",
    "    # make iterator for saving substacks\n",
    "    t = 1\n",
    "    counter = 0\n",
    "    \n",
    "    # iterate through each file\n",
    "    f_list = []\n",
    "    for i=1:num_files\n",
    "        \n",
    "        f_list = vcat(f_list,files[i])\n",
    "        \n",
    "        # count time steps and save output\n",
    "        if i > 1\n",
    "            datetime = datetime + Second(seconds_per_file)\n",
    "        end   \n",
    "        if datetime <= output_times[t] && datetime+Second(seconds_per_file) > output_times[t]\n",
    "            t = t + 1\n",
    "            fname = string(\"correlations_\",datetime,\".jld2\")\n",
    "            #println(\"output as\",fname)\n",
    "\n",
    "            if fname == f_desired\n",
    "                return f_list\n",
    "            end\n",
    "            f_list = []\n",
    "        end\n",
    "\n",
    "        \n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function workflow_new_files(files,output_times,f_desired,samples_per_file=[])\n",
    "    \n",
    "    # read the first file and collect metadata\n",
    "    N = read_nodal(\"segy\", files[1])\n",
    "    if isempty(samples_per_file) == true\n",
    "        samples_per_file = N.info[\"orignx\"]\n",
    "    end\n",
    "    seconds_per_file = samples_per_file/N.fs[1]\n",
    "    num_files = length(files)\n",
    "    \n",
    "    # iterate through each file\n",
    "    next_output_ind = 1\n",
    "    outfiles = []\n",
    "    for i=1:num_files\n",
    "\n",
    "        # read the file\n",
    "        datetime = DateTime(split(split(files[i],\"Q_\")[2],\".\")[1],\"yymmddHHMMSS\")+Year(2000)\n",
    "        outfiles = vcat(outfiles,datetime)\n",
    "        \n",
    "        #println(\"Summed correlations for \"*files[i])\n",
    "        #flush(stdout)\n",
    "\n",
    "        # get the next output time\n",
    "        bf = datetime .<= output_times\n",
    "        af = (datetime + Second(30)) .> output_times\n",
    "        output_ind = findfirst(bf+af .== 2)\n",
    "        if output_ind != nothing\n",
    "            next_output_ind = output_ind + 1\n",
    "        end\n",
    "\n",
    "        # check if there's about to be a gap \n",
    "        next_datetime = datetime\n",
    "        if i < num_files\n",
    "            next_datetime = DateTime(split(split(files[i+1],\"Q_\")[2],\".\")[1],\"yymmddHHMMSS\")+Year(2000)\n",
    "        end\n",
    "        gap = next_datetime-datetime\n",
    "\n",
    "        # if the gap would push us past the next output time, write a file\n",
    "        if gap > Second(30) && datetime + gap > output_times[next_output_ind]\n",
    "            print(\"Gap (\",gap.value/1000/60,\" mins) at \",datetime,\" \\n\")\n",
    "            fname = string(\"correlations_\",datetime,\".jld2\")\n",
    "            #println(\"Saved \",fname,\" (last file: \"*files[i]*\")\")\n",
    "            #flush(stdout)\n",
    "            outfiles = []\n",
    "        end\n",
    "\n",
    "        # if within one file duration of an output time, write a file\n",
    "        if sum(bf+af .== 2) == 1\n",
    "            fname = string(\"correlations_\",datetime,\".jld2\")\n",
    "            if fname == f_desired\n",
    "                println(outfiles)\n",
    "                flush(stdout)\n",
    "            end    \n",
    "            #println(\"Saved \",fname,\" (last file: \"*files[i]*\")\")\n",
    "            #flush(stdout)\n",
    "            outfiles = []\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af683609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "List all the input files to produce one of the 1-hour 'good' files (old workflow)\n",
    "\n",
    "\"\n",
    "\n",
    "\n",
    "# list all 1khz and resampled Greenland files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled\"\n",
    "files_1khz = glob(\"1kHz/*\",path_1khz)\n",
    "files_resampled = glob(\"*\",path_resampled)\n",
    "files = cat(files_1khz,files_resampled,dims=1)\n",
    "N = read_nodal(\"segy\", files[1])\n",
    "\n",
    "# subset to some specific files\n",
    "files = files[2:end]\n",
    "\n",
    "# set substack timing and output path for 1 khz files\n",
    "substack_time = Minute(60)\n",
    "Ns,Nf = read_nodal(\"segy\", files[1]), read_nodal(\"segy\", files[end])\n",
    "start_datetime,end_datetime = get_datetime(Ns),get_datetime(Nf)\n",
    "output_times = start_datetime+substack_time:substack_time:end_datetime\n",
    "out_path = string(\"/fd1/solinger/correlations/fk_1500_2250/\")\n",
    "\n",
    "# set which output file for which we want to get the corresponding input files\n",
    "f_desired = \"correlations_2019-07-08T15:12:20.jld2\"\n",
    "\n",
    "# correlate 1khz files\n",
    "f_list = workflow_old_files(files,output_times,f_desired);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f7d5baa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[DateTime(\"2019-07-05T13:12:20\"), DateTime(\"2019-07-05T13:12:50\"), DateTime(\"2019-07-05T13:13:20\"), DateTime(\"2019-07-05T13:13:50\"), DateTime(\"2019-07-05T13:14:20\"), DateTime(\"2019-07-05T13:14:50\"), DateTime(\"2019-07-05T13:15:20\"), DateTime(\"2019-07-05T13:15:50\"), DateTime(\"2019-07-05T13:16:35\"), DateTime(\"2019-07-05T13:17:05\"), DateTime(\"2019-07-05T13:17:35\"), DateTime(\"2019-07-05T13:18:05\"), DateTime(\"2019-07-05T13:18:35\"), DateTime(\"2019-07-05T13:19:05\"), DateTime(\"2019-07-05T13:19:35\"), DateTime(\"2019-07-05T13:20:05\"), DateTime(\"2019-07-05T13:20:35\"), DateTime(\"2019-07-05T13:21:05\"), DateTime(\"2019-07-05T13:21:35\"), DateTime(\"2019-07-05T13:22:05\")]\n",
      "Gap (22.95 mins) at 2019-07-05T16:27:35 \n",
      "Gap (88.76666666666667 mins) at 2019-07-05T21:33:31 \n",
      "Gap (81.51666666666667 mins) at 2019-07-06T17:39:51 \n"
     ]
    }
   ],
   "source": [
    "\"\n",
    "\n",
    "List all the input files to produce the 10 minute recent run\n",
    "\n",
    "\"\n",
    "\n",
    "\n",
    "# list all 1khz and resampled Greenland files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled\"\n",
    "files_1khz = glob(\"1kHz/*\",path_1khz)\n",
    "files_resampled = glob(\"*\",path_resampled)\n",
    "files = cat(files_1khz,files_resampled,dims=1)\n",
    "N = read_nodal(\"segy\", files[1])\n",
    "\n",
    "# subset to some specific files\n",
    "files = files[2:end]\n",
    "\n",
    "# set substack timing and output path for 1 khz files\n",
    "substack_time = Minute(10)\n",
    "Ns,Nf = read_nodal(\"segy\", files[1]), read_nodal(\"segy\", files[end])\n",
    "start_datetime,end_datetime = get_datetime(Ns),get_datetime(Nf)\n",
    "output_times = start_datetime+substack_time:substack_time:end_datetime\n",
    "\n",
    "# set which output file for which we want to get the corresponding input files\n",
    "f_desired = \"correlations_2019-07-05T13:22:05.jld2\"\n",
    "\n",
    "# correlate 1khz files\n",
    "workflow_new_files(files,output_times,f_desired);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0976e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131220.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131250.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131320.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131350.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131420.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131450.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131520.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131635.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131705.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131735.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131805.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131835.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131905.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131935.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132005.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132035.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132105.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132135.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132205.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132235.sgy\n",
      "Saved (last file: /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132235.sgy)\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132305.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132335.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132405.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132435.sgy\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] Array",
      "    @ ./boot.jl:461 [inlined]",
      "  [2] Array",
      "    @ ./boot.jl:469 [inlined]",
      "  [3] similar",
      "    @ ./array.jl:378 [inlined]",
      "  [4] similar",
      "    @ ./abstractarray.jl:795 [inlined]",
      "  [5] _unsafe_getindex(::IndexLinear, ::Matrix{Float32}, ::Base.Slice{Base.OneTo{Int64}}, ::Base.Slice{Base.OneTo{Int64}}, ::Int64)",
      "    @ Base ./multidimensional.jl:887",
      "  [6] _getindex",
      "    @ ./multidimensional.jl:875 [inlined]",
      "  [7] getindex",
      "    @ ./abstractarray.jl:1241 [inlined]",
      "  [8] filt!",
      "    @ ~/.julia/packages/DSP/q9iEF/src/Filters/filt.jl:74 [inlined]",
      "  [9] filtfilt(f::DSP.Filters.SecondOrderSections{Float32, Float32}, x::SubArray{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})",
      "    @ DSP.Filters ~/.julia/packages/DSP/q9iEF/src/Filters/filt.jl:325",
      " [10] filtfilt",
      "    @ ~/.julia/packages/DSP/q9iEF/src/Filters/filt.jl:336 [inlined]",
      " [11] bandpass!(A::Matrix{Float32}, freqmin::Int64, freqmax::Int64, fs::Float64; corners::Int64, zerophase::Bool)",
      "    @ SeisNoise ~/.julia/packages/SeisNoise/E7Pc3/src/filter.jl:57",
      " [12] bandpass!(N::NodalData, freqmin::Int64, freqmax::Int64; corners::Int64, zerophase::Bool)",
      "    @ SeisNoise ~/.julia/packages/SeisNoise/E7Pc3/src/filter.jl:98",
      " [13] workflow_old(files::Vector{String}, cc_len::Int64, maxlag::Int64, freqmin::Int64, freqmax::Int64, fs::Int64, cmin::Int64, cmax::Int64, sgn::String, time_norm::String, chans::Vector{Int64}, output_times::StepRange{DateTime, Minute}, out_path::String, geometry::String, whitening::Int64, samples_per_file::Int64, mode::String, device::Int64)",
      "    @ Main ~/notebooks/greenlanDAS/correlation/Workflow_old.jl:73",
      " [14] top-level scope",
      "    @ In[75]:50"
     ]
    }
   ],
   "source": [
    "\"\n",
    "\n",
    "Run old workflow with those same input files and same parameters\n",
    "\n",
    "\"\n",
    "\n",
    "# list all 1khz and resampled Greenland files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled\"\n",
    "files_1khz = glob(\"1kHz/*\",path_1khz)\n",
    "files_resampled = glob(\"*\",path_resampled)\n",
    "files = cat(files_1khz,files_resampled,dims=1)\n",
    "N = read_nodal(\"segy\", files[1])\n",
    "\n",
    "# choose channels\n",
    "chan_start = 331\n",
    "chan_end = 1361\n",
    "chans = [chan_start,chan_end]\n",
    "\n",
    "# set filter band\n",
    "freqmin,freqmax = 1,100\n",
    "fs = 400\n",
    "\n",
    "# set frequeny and time normalization\n",
    "whitening = 0\n",
    "time_norm = \"1bit\"\n",
    "\n",
    "# set windowing parameters\n",
    "cc_len = 10\n",
    "maxlag = 1\n",
    "\n",
    "# choose fk filter bounds\n",
    "cmin,cmax = 3500,4250\n",
    "sgn = \"pos\"\n",
    "\n",
    "# indicate cable geometry (linear or u-shaped)\n",
    "geometry = \"l\"\n",
    "\n",
    "# choose a subset of files \n",
    "run_files = files[2:end]\n",
    "\n",
    "# set substack timing and output path for 1 khz files\n",
    "substack_time = Minute(10)\n",
    "Ns,Nf = read_nodal(\"segy\", run_files[1]), read_nodal(\"segy\", run_files[end])\n",
    "start_datetime,end_datetime = get_datetime(Ns),get_datetime(Nf)\n",
    "output_times = start_datetime+substack_time:substack_time:end_datetime\n",
    "out_path = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/\")\n",
    "\n",
    "# correlate 1khz files\n",
    "NC = workflow_old(run_files,cc_len,maxlag,freqmin,freqmax,fs,cmin,cmax,sgn,\n",
    "               time_norm,chans,output_times,out_path,geometry,whitening,30000,\"auto\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ccf3ad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131220.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131250.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131320.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131350.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131420.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131450.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131520.sgy\n",
      "Did not correlate /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131550.sgy (too few samples)\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131635.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131705.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131735.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131805.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131835.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131905.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705131935.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132005.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132035.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132105.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132135.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132205.sgy\n",
      "Saved (last file: /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132205.sgy)\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132235.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132305.sgy\n",
      "Summed correlations for /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705132335.sgy\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] Array",
      "   @ ./boot.jl:459 [inlined]",
      " [2] IdDict",
      "   @ ./iddict.jl:30 [inlined]",
      " [3] IdDict",
      "   @ ./iddict.jl:48 [inlined]",
      " [4] deepcopy",
      "   @ ./deepcopy.jl:26 [inlined]",
      " [5] resample_kernel(x::Matrix{Float32}, rate::Float32)",
      "   @ SeisNoise ~/.julia/packages/SeisNoise/E7Pc3/src/filter.jl:485",
      " [6] resample!(N::NodalData, fs::Int64)",
      "   @ Main ~/notebooks/greenlanDAS/correlation/Nodal.jl:22",
      " [7] workflow(files::Vector{String}, cc_len::Int64, maxlag::Int64, freqmin::Int64, freqmax::Int64, fs::Int64, cmin::Int64, cmax::Int64, sgn::String, time_norm::String, chans::Vector{Int64}, output_times::StepRange{DateTime, Minute}, out_path::String, geometry::String, whitening::Int64, samples_per_file::Int64, mode::String, device::Int64)",
      "   @ Main ~/notebooks/greenlanDAS/correlation/Workflow.jl:130",
      " [8] top-level scope",
      "   @ In[69]:50"
     ]
    }
   ],
   "source": [
    "\"\n",
    "\n",
    "Run new workflow with those same input files and same parameters\n",
    "\n",
    "\"\n",
    "\n",
    "# list all 1khz and resampled Greenland files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled\"\n",
    "files_1khz = glob(\"1kHz/*\",path_1khz)\n",
    "files_resampled = glob(\"*\",path_resampled)\n",
    "files = cat(files_1khz,files_resampled,dims=1)\n",
    "N = read_nodal(\"segy\", files[1])\n",
    "\n",
    "# choose channels\n",
    "chan_start = 331\n",
    "chan_end = 2391\n",
    "chans = [chan_start,chan_end]\n",
    "\n",
    "# set filter band\n",
    "freqmin,freqmax = 1,100\n",
    "fs = 400\n",
    "\n",
    "# set frequeny and time normalization\n",
    "whitening = 0\n",
    "time_norm = \"1bit\"\n",
    "\n",
    "# set windowing parameters\n",
    "cc_len = 10\n",
    "maxlag = 1\n",
    "\n",
    "# choose fk filter bounds\n",
    "cmin,cmax = 3500,4250\n",
    "sgn = \"pos\"\n",
    "\n",
    "# indicate cable geometry (linear or u-shaped)\n",
    "geometry = \"u\"\n",
    "\n",
    "# choose a subset of files \n",
    "run_files = files[2:end]\n",
    "\n",
    "# set substack timing and output path for 1 khz files\n",
    "substack_time = Minute(10)\n",
    "Ns,Nf = read_nodal(\"segy\", run_files[1]), read_nodal(\"segy\", run_files[end])\n",
    "start_datetime,end_datetime = get_datetime(Ns),get_datetime(Nf)\n",
    "output_times = start_datetime+substack_time:substack_time:end_datetime\n",
    "out_path = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/\")\n",
    "\n",
    "# correlate 1khz files\n",
    "NC = workflow(run_files,cc_len,maxlag,freqmin,freqmax,fs,cmin,cmax,sgn,\n",
    "               time_norm,chans,output_times,out_path,geometry,whitening,30000,\"auto_cross\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5b24690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\n",
    "\n",
    "Compare the output file produced by the new and old workflows\n",
    "\n",
    "\"\n",
    "\n",
    "# read file\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/correlations_2019-07-05T13:22:20_old.jld2\")\n",
    "C1 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "clean_up!(C1,10.,13.)\n",
    "\n",
    "# read file\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/correlations_2019-07-05T13:22:05.jld2\")\n",
    "C2 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "clean_up!(C2,10.,13.)\n",
    "\n",
    "# compare output\n",
    "C2.corr[:,1:1030] == C1.corr[:,1:1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422ec08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "\"\n",
    "\n",
    "Compare the output file produced by the new workflow and from actual run\n",
    "\n",
    "\"\n",
    "\n",
    "# read file\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/correlations_2019-07-05T13:22:05.jld2\")\n",
    "C2 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "\n",
    "# read file\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/10_min/correlations_2019-07-05T13:22:05.jld2\")\n",
    "C3 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "    \n",
    "# compare output\n",
    "println(C3.corr == C2.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16db5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "Compare the output file produced by the new workflow and the output file originally produced\n",
    "\n",
    "\"\n",
    "\n",
    "# read file\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/correlations_2019-07-08T18:30:22.jld2\")\n",
    "C1 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "#clean_up!(C1,10.,13.)\n",
    "\n",
    "\n",
    "# read file\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/correlations_2019-07-08T15:12:20.jld2\")\n",
    "C2 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "#clean_up!(C2,10.,13.)\n",
    "\n",
    "# compare output\n",
    "C2.corr[:,1:1030] == C1.corr[:,1:1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd07627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "Manually restack the 30-second output files \n",
    "\n",
    "\"\n",
    "\n",
    "\n",
    "# list all correlation files\n",
    "files = glob(\"*.jld2\",\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/\")\n",
    "files = files[BitVector(1 .- contains.(files,\"autocorrelation\"))]\n",
    "files = files[BitVector(1 .- contains.(files,\"datetimes\"))]\n",
    "files = files[BitVector(1 .- contains.(files,\"dv\"))]\n",
    "files = files[BitVector(1 .- contains.(files,\"old\"))]\n",
    "\n",
    "\n",
    "# get some stuff\n",
    "C = JLD2.load(files[2])[\"NodalCorrData\"]\n",
    "ns = size(C.corr,1)\n",
    "ncorr = size(C.corr,2)\n",
    "nstacks = size(files,1)\n",
    "    \n",
    "# get some stuff\n",
    "all_autocorrs = zeros(ns,ncorr)\n",
    "C3 = JLD2.load(files[1])[\"NodalCorrData\"]\n",
    "for i=2:size(files,1)\n",
    "    C3.corr += JLD2.load(files[i])[\"NodalCorrData\"].corr\n",
    "end\n",
    "clean_up!(C3,10.,13.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "Compare the output file produced by the new and old workflows with restacked output\n",
    "\n",
    "\"\n",
    "\n",
    "# read restacked file\n",
    "all_autocorrs = JLD2.load(fname)[\"autocorrs\"]\n",
    "\n",
    "# read old workflow stack\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/correlations_2019-07-08T17:40:22_old.jld2\")\n",
    "C1 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "clean_up!(C1,10.,13.)\n",
    "\n",
    "# read new workflow stack\n",
    "file = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/correlations_2019-07-08T17:40:22.jld2\")\n",
    "C2 = JLD2.load(file)[\"NodalCorrData\"]\n",
    "clean_up!(C2,10.,13.)\n",
    "\n",
    "# compare output\n",
    "C2.corr[:,1:1030] == C3.corr[:,1:1030]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ada1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "Manually restack the 30-second output files \n",
    "\n",
    "\"\n",
    "\n",
    "\n",
    "# list all correlation files\n",
    "files = glob(\"*.jld2\",\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/\")\n",
    "files = files[BitVector(1 .- contains.(files,\"autocorrelation\"))]\n",
    "files = files[BitVector(1 .- contains.(files,\"datetimes\"))]\n",
    "files = files[BitVector(1 .- contains.(files,\"dv\"))]\n",
    "files = files[BitVector(1 .- contains.(files,\"old\"))]\n",
    "\n",
    "# get some stuff\n",
    "C = JLD2.load(files[2])[\"NodalCorrData\"]\n",
    "ns = size(C.corr,1)\n",
    "ncorr = size(C.corr,2)\n",
    "nstacks = size(files,1)\n",
    "\n",
    "# get datetimes of each file\n",
    "datetimes = []\n",
    "for i=1:size(files,1)\n",
    "    datetimes = vcat(datetimes,DateTime(string(\"2019\"*split(split(files[i],\"_2019\")[2],\".jld2\")[1])))\n",
    "end\n",
    "\n",
    "# choose restack parameters\n",
    "restack = 1\n",
    "restack_interval = Minute(9)\n",
    "start_datetime = datetimes[1]-Minute(datetimes[1])-Second(datetimes[1])\n",
    "end_datetime = datetimes[end]+Hour(1)-Minute(datetimes[end])-Second(datetimes[end])\n",
    "restack_lims = start_datetime:restack_interval:end_datetime\n",
    "bands = [[10. 13.];]\n",
    "\n",
    "# get a list of files that should be stacked together\n",
    "stack_files = []\n",
    "for i=1:size(restack_lims,1)-1\n",
    "    stack_ind = (datetimes .> restack_lims[i]) .& (datetimes .< restack_lims[i+1])\n",
    "    append!(stack_files,[files[stack_ind]])\n",
    "end\n",
    "nstacks = size(stack_files,1)\n",
    "\n",
    "# make output data matrix\n",
    "all_autocorrs = zeros(ns,ncorr,nstacks,size(bands,1))\n",
    "\n",
    "# set a useful counter and read first file\n",
    "for i=1:size(stack_files,1)\n",
    "\n",
    "    # if there's not a gap\n",
    "    if size(stack_files[i],1) > 0\n",
    "        \n",
    "        # stack each file\n",
    "        for j=1:size(stack_files[i],1)\n",
    "            if j == 1\n",
    "                C = JLD2.load(stack_files[i][j])[\"NodalCorrData\"]\n",
    "            else\n",
    "                C.corr += JLD2.load(stack_files[i][j])[\"NodalCorrData\"].corr\n",
    "            end\n",
    "        end\n",
    "   \n",
    "        # load stack into data structure\n",
    "        for k=1:size(bands,1)\n",
    "\n",
    "            # postprocessing\n",
    "            C_filt = deepcopy(C)\n",
    "            clean_up!(C_filt,bands[k,1],bands[k,2])\n",
    "\n",
    "            # fill output\n",
    "            all_autocorrs[:,:,i,k] = C_filt.corr\n",
    "        end\n",
    "            \n",
    "    end\n",
    "end\n",
    "\n",
    "# save the autocorrelations\n",
    "fname = string(\"/fd1/solinger/correlations/fk_3500_4250/no_whitening/debug/stacks/test_restack.jld2\")\n",
    "JLD2.save(fname,Dict(\"autocorrs\"=>all_autocorrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_autocorrs[:,:,5,1]+all_autocorrs[:,:,4,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3086a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. read an hour with no gaps from the old, 'good' correlations\n",
    "\n",
    "# 2. rerun new algorithm with same files, same parameters, and 60 minute stacking\n",
    "\n",
    "# 3. rerun new algorithm with same files, same parameters, and 10 minute stacking\n",
    "# and restack after correlation\n",
    "\n",
    "# if 1. and 2. are not the same:\n",
    "# the issue is in the core workflow function (file handling, preprocessing, \n",
    "# fk function, correlation, stacking)\n",
    "\n",
    "# if 1. and 2. are the same but 2. and 3. are not:\n",
    "# the issue is in the postprocessing and/or post-correlation stacking\n",
    "\n",
    "# if 1. 2. and 3. are the same:\n",
    "# the issue is in the dv/v calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed32d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "NOTES\n",
    "\n",
    "\"\n",
    "\n",
    "# Do old and new workflow, for a single input file and identical parameters, give identical output? \n",
    "#     YES\n",
    "#\n",
    "# Do old and new workflow with u geometry and \"auto\", for a single input file, \n",
    "# give identical output for corresponding channels? \n",
    "#     YES\n",
    "#\n",
    "# Do old and new workflow with u geometry and \"auto_cross\", for a single input file, \n",
    "# give identical output for corresponding channels? \n",
    "#     NO <-------- START HERE\n",
    "#     I was dropping the 3rd dimension and losing the other 10-second slices in apply_fk_u_geo!\n",
    "#\n",
    "# Do apply_fk_l_geo and apply_fk_u_geo give identical output, for a single input file, \n",
    "# give identical output for corresponding channels? \n",
    "#     YES --> issue IS in \"auto_cross\"\n",
    "#\n",
    "# For a 10-minute stack, do old and new workflow with u geometry and \"auto_cross\"\n",
    "# give identical output for corresponding channels? \n",
    "#\n",
    "#\n",
    "# Does stacking during correlation and stacking output files after correlation \n",
    "# give identical output for corresponding channels? \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e20fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "Figure out what's wrong with the the auto_cross method\n",
    "\n",
    "\"\n",
    "\n",
    "function autocorrelate(NF::NodalFFTData,maxlag::Int)\n",
    "    if length(size(NF.fft)) == 3\n",
    "        Nt,Nc,Nw = size(NF.fft)\n",
    "    elseif length(size(NF.fft)) == 2\n",
    "        Nt,Nc = size(NF.fft)\n",
    "        Nw = 1\n",
    "    end\n",
    "    Ncorr = Nc \n",
    "    Cout = similar(NF.fft,maxlag * 2 + 1,Ncorr,Nw)\n",
    "    cstart = 0\n",
    "    cend = 0\n",
    "    FFT1 = NF.fft\n",
    "    FFT2 = NF.fft\n",
    "    corrT = irfft(conj.(FFT1) .* FFT2,NF.ns,1)\n",
    "    println(corrT[end-1,end])\n",
    "    \n",
    "    # return corr[-maxlag:maxlag]\n",
    "    t = vcat(0:Int(NF.ns / 2)-1, -Int(NF.ns / 2):-1)\n",
    "    ind = findall(abs.(t) .<= maxlag)\n",
    "    newind = fftshift(ind,1) \n",
    "    Cout .= corrT[newind,:,:] \n",
    "  \n",
    "    return Cout\n",
    "end\n",
    "\n",
    "function autocorrelate_cross(NF::NodalFFTData,maxlag::Int,split)\n",
    "    if length(size(NF.fft)) == 3\n",
    "        Nt,Nc,Nw = size(NF.fft)\n",
    "    elseif length(size(NF.fft)) == 2\n",
    "        Nt,Nc = size(NF.fft)\n",
    "        Nw = 1\n",
    "    end\n",
    "    \n",
    "    # get legs\n",
    "    NF_leg_1 = NF.fft[:,1:split,:]\n",
    "    NF_leg_2 = reverse(NF.fft[:,split:end,:],dims=2)\n",
    "    Ncorr = (split)*3\n",
    "    Cout = similar(NF.fft,maxlag * 2 + 1,Ncorr,Nw)\n",
    "    \n",
    "    # leg 1 auto\n",
    "    corrT_leg_1 = irfft(conj.(NF_leg_1) .* NF_leg_1,NF.ns,1)\n",
    "    println(corrT_leg_1[end-1,end])\n",
    "\n",
    "    # leg 2 auto\n",
    "    corrT_leg_2 = irfft(conj.(NF_leg_2) .* NF_leg_2,NF.ns,1)\n",
    "\n",
    "    # cros \"auto\"\n",
    "    corrT_cross = irfft(conj.(NF_leg_1) .* NF_leg_2,NF.ns,1)\n",
    "\n",
    "    # return corr[-maxlag:maxlag]\n",
    "    t = vcat(0:Int(NF.ns / 2)-1, -Int(NF.ns / 2):-1)\n",
    "    ind = findall(abs.(t) .<= maxlag)\n",
    "    newind = fftshift(ind,1) \n",
    "    \n",
    "    # fill output\n",
    "    Cout[:,1:split,:] .= corrT_leg_1[newind,:,:] \n",
    "    Cout[:,split+1:2*split,:] .= corrT_leg_2[newind,:,:] \n",
    "    Cout[:,2*split+1:end,:] .= corrT_cross[newind,:,:] \n",
    "\n",
    "    return Cout\n",
    "end\n",
    "\n",
    "\n",
    "# set common parameters for both methods\n",
    "cc_len = 10\n",
    "maxlag = 1\n",
    "fs = 400\n",
    "freqmin, freqmax = 1,100\n",
    "sgn = \"pos\"\n",
    "cmin = 3500\n",
    "cmax = 4250\n",
    "\n",
    "# test the base autocorrelate method\n",
    "chan_start = 331\n",
    "chan_end = 1361\n",
    "chans = [chan_start,chan_end]\n",
    "num_chans = chans[2]-chans[1]+1\n",
    "N = read_nodal(\"segy\", files[10])[chans[1]:chans[end]] |> cu\n",
    "N = apply_fk_l_geo(N,cmin,cmax,sgn,num_chans)\n",
    "sliced_data = slice(N,cc_len)\n",
    "NP = NodalProcessedData(N.n,size(sliced_data)[1],N.ox,N.oy,N.oz,N.info,N.id,N.name,\n",
    "           N.loc,fs*ones(N.n),N.gain,Float64(freqmin),Float64(freqmax),cc_len,\"1bit\",\n",
    "           N.resp,N.units,N.src,N.misc,N.notes,N.t,sliced_data)\n",
    "NF1 = rfft(NP,[1])\n",
    "corr = autocorrelate(NF1,Int64(maxlag*NF1.fs[1]))\n",
    "corr = real(Array(sum(corr,dims=3)))\n",
    "\n",
    "# define split point for u-shaped geometry\n",
    "chan_start = 331\n",
    "chan_end = 2391\n",
    "chans = [chan_start,chan_end]\n",
    "num_chans = chans[2]-chans[1]+1\n",
    "split_pt = num_chans - div(num_chans,2)\n",
    "N1 = read_nodal(\"segy\", files[10])[chans[1]:chans[end]] |> cu\n",
    "N1 = apply_fk_u_geo(N1,cmin,cmax,sgn,split_pt)\n",
    "sliced_data = slice(N1,cc_len)\n",
    "NP1 = NodalProcessedData(N1.n,size(sliced_data)[1],N1.ox,N1.oy,N1.oz,N1.info,N1.id,N1.name,\n",
    "           N1.loc,fs*ones(N1.n),N1.gain,Float64(freqmin),Float64(freqmax),cc_len,\"1bit\",\n",
    "           N1.resp,N1.units,N1.src,N1.misc,N1.notes,N1.t,sliced_data)\n",
    "NF2 = rfft(NP1,[1])\n",
    "corr1 = autocorrelate_cross(NF2,Int64(maxlag*NF2.fs[1]),split_pt)\n",
    "corr1 = real(Array(sum(corr1,dims=3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
