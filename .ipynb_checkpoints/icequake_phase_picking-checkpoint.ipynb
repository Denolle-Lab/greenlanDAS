{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e845979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seisbench.models as sbm\n",
    "from obspy.signal.filter import bandpass\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from obspy.signal.trigger import recursive_sta_lta\n",
    "from obspy_local.obspy_local.io.segy.core import _read_segy\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.signal import spectrogram, butter, sosfilt, decimate, detrend, resample, resample_poly\n",
    "from scipy.signal.windows import hann\n",
    "from scipy.ndimage import gaussian_filter, binary_dilation,label\n",
    "from scipy.ndimage import sum as imsum\n",
    "import scipy\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from numpy.fft import rfftn,irfftn,fftfreq,rfftfreq,fftshift,fft2,ifft2\n",
    "import glob\n",
    "import matplotlib. pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime as datetime\n",
    "import pickle\n",
    "import collections\n",
    "import multiprocessing\n",
    "import types\n",
    "import copy\n",
    "import struct\n",
    "import pickle\n",
    "import time\n",
    "from PIL import Image\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "from ELEP.elep.ensemble_learners import ensemble_regressor_cnn\n",
    "import ELEP.elep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk(data,fs,dx,cmin,cmax,sgn):\n",
    "\n",
    "    # get some values\n",
    "    ns = data.shape[0]\n",
    "    n = data.shape[1]\n",
    "\n",
    "    # take rfft\n",
    "    data_fft = fft2(data)    \n",
    "\n",
    "    # note that rfft only reduces the fft size for the first dimension, so we use\n",
    "    # rfftfreq for k0 but fftfreq (with fftshift) for f0\n",
    "    k0 = fftshift(fftfreq(n,dx))\n",
    "    f0 = fftshift(fftfreq(ns,1./fs))\n",
    "    K,F = np.meshgrid(k0,f0)\n",
    "    C = F/K \n",
    "    \n",
    "    # build filter in fk domain\n",
    "    filt = np.zeros(data_fft.shape)\n",
    "    if sgn == \"both\":\n",
    "        filt[np.logical_and(((C<-cmin) & (C>-cmax)),((C>cmin) & (C<cmax)))] = 1.\n",
    "    elif sgn == \"neg\":\n",
    "        filt[np.logical_and(C>cmin,C<cmax)] = 1.\n",
    "    elif sgn == \"pos\":\n",
    "        filt[np.logical_and(C<-cmin,C>-cmax)] = 1.\n",
    "\n",
    "    # smooth filter, apply to transformed data, and invert transform\n",
    "    filt = gaussian_filter(filt,3) # blur the filter a little to reduce Gibbs ringing\n",
    "    filt_fft = fftshift(filt)*data_fft\n",
    "    filt_data = ifft2(filt_fft,s=data.shape).astype('float')\n",
    "    return filt_data\n",
    "\n",
    "\n",
    "def normalize_input(sliced_data,mode):\n",
    "    # normalize by standard deviation\n",
    "    if mode == \"std\":\n",
    "        std_devs = np.expand_dims(np.std(sliced_data,axis=2),axis=2)\n",
    "        data_norm = np.divide(sliced_data,std_devs)\n",
    "        data_norm[np.isnan(data_norm)] = 0\n",
    "    elif mode == \"max\":\n",
    "        max_vals = np.expand_dims(np.max(np.abs(sliced_data),axis=2),axis=2)\n",
    "        data_norm = np.divide(sliced_data,max_vals)\n",
    "        data_norm[np.isnan(data_norm)] = 0\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "def taper(data_norm,taper_len):\n",
    "    # cosine taper the normalized data\n",
    "    taper = 0.5 * (1 + np.cos(np.linspace(np.pi, 2 * np.pi, taper_len)))\n",
    "    data_norm[:,:,:taper_len] *= taper; \n",
    "    data_norm[:,:,-taper_len:] *= taper[::-1];\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "def apply_model(data_norm,model,eqt,device):\n",
    "    data_torch = torch.Tensor(data_norm)\n",
    "    torch_predictions = eqt(data_torch.to(device))\n",
    "    model_detection_pred = torch_predictions[0].detach().cpu().numpy() # detection\n",
    "    #model_p_wave_pred = torch_predictions[1].detach().cpu().numpy() # p-wave\n",
    "    #model_s_wave_pred = torch_predictions[2].detach().cpu().numpy() # s-wave\n",
    "    return model_detection_pred\n",
    "\n",
    "\n",
    "def shift_stack(pred_combined,spacing,vel,fs,num_chan):\n",
    "    delay = spacing/vel\n",
    "    shift = int(np.floor(delay*fs))\n",
    "    if shift < 1:\n",
    "        shift = 1\n",
    "    migrated_pred = np.zeros((num_chan,6000+shift*num_chan))\n",
    "    for j in range(num_chan):\n",
    "        shift_win_start = j*shift\n",
    "        start_idx = -1-6000-j*shift\n",
    "        end_idx = -1-j*shift\n",
    "        migrated_pred[j,start_idx:start_idx+6000] = pred_combined[j,:]\n",
    "\n",
    "    # fill 0-valued regions with mean\n",
    "    migrated_pred[migrated_pred == 0] = np.mean(pred_combined[pred_combined!=0])\n",
    "    #migrated_pred[migrated_pred == 0] = 0\n",
    "\n",
    "    # sum all p-wave predictions across channel\n",
    "    condensed_migr_pred = np.sum(migrated_pred,axis=0)\n",
    "\n",
    "    return condensed_migr_pred\n",
    "\n",
    "\n",
    "def predict_each_window(reshaped_data,models,eqts,fs,channels,blnd,taper_len,vel):\n",
    "    \n",
    "    # get some stuff\n",
    "    num_chan = channels[1]-channels[0]\n",
    "    ns_input = 6000 # must be 6000 for eqt model\n",
    "    \n",
    "    # make container for the data (nwindow x ncomponent x nsample) and output\n",
    "    pred_combined = np.ones((num_chan, ns_input), dtype = np.float32) \n",
    "    pred = []\n",
    "    sliced_data = np.zeros((num_chan,3,int(ns_input)),dtype=np.float32)\n",
    "\n",
    "    # iterate through each window of data\n",
    "    #for w in [37]:\n",
    "    for w in range(reshaped_data.shape[0]):\n",
    "\n",
    "        # reset value of output container\n",
    "        pred_combined[:,:] = 1\n",
    "\n",
    "        # put the data into a nwindow x ncomponent x nsample container\n",
    "        sliced_data[:,:,:] = 0 \n",
    "        sliced_data[:,1,:] = np.transpose(reshaped_data[w,:,0:channels[1]-channels[0]])\n",
    "\n",
    "        # normalize by standard deviation\n",
    "        data_std_norm = normalize_input(sliced_data,\"std\")\n",
    "        data_max_norm = normalize_input(sliced_data,\"max\")\n",
    "\n",
    "        # cosine taper the normalized data\n",
    "        data_std_norm = taper(data_std_norm,taper_len)\n",
    "        data_max_norm = taper(data_max_norm,taper_len)\n",
    "\n",
    "        # start timer\n",
    "        #t = time.time()\n",
    "\n",
    "        # iterate through list of models\n",
    "        for i,model in enumerate(models):\n",
    "\n",
    "            # detection\n",
    "            if model == \"original\":\n",
    "                model_det_pred = apply_model(data_std_norm,model,eqts[i],device)\n",
    "            else:\n",
    "                model_det_pred = apply_model(data_max_norm,model,eqts[i],device)\n",
    "            \n",
    "            # normalize predictions\n",
    "            pred_det_norm = model_det_pred/np.expand_dims(np.max(np.abs(model_det_pred),axis=(1)),axis=(1))\n",
    "            #pred_p_norm = model_p_pred/np.expand_dims(np.max(np.abs(model_p_pred),axis=(1)),axis=(1))\n",
    "\n",
    "            # add to sum of predictions from each model\n",
    "            pred_combined[:,:] *= pred_det_norm\n",
    "\n",
    "        # stop timer\n",
    "        #timer = time.time() - t\n",
    "        #print(\"Applied all models in \",timer,\"s\")\n",
    "\n",
    "        # zero out edge predictions\n",
    "        pred_combined[:,:blnd[0]] = 0\n",
    "        pred_combined[:,-blnd[1]:] = 0\n",
    "\n",
    "        # align based on a few possible p-wave velocities\n",
    "        for j,vel in enumerate(vels):\n",
    "            vel_pred = shift_stack(pred_combined,spacing,vel,fs,num_chan)\n",
    "            if j == 0:\n",
    "                max_val = np.max(vel_pred)\n",
    "                migr_pred = vel_pred\n",
    "            if j > 0 and np.max(vel_pred) > max_val:\n",
    "                max_val = np.max(vel_pred)\n",
    "                migr_pred = vel_pred\n",
    "\n",
    "        # detrend\n",
    "        migr_pred = detrend(migr_pred,type=\"constant\")\n",
    "        migr_pred = detrend(migr_pred)\n",
    "        \n",
    "        # add to output\n",
    "        pred.append(migr_pred)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def trigger(predictions,threshold):\n",
    "\n",
    "    detection_times = []\n",
    "    detection_predictions = []\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "\n",
    "        # throw away the first 0.5 second window of each file\n",
    "        predictions[i][0][:] = 0\n",
    "\n",
    "        for j in range(len(predictions[i])):  \n",
    "            detections = predictions[i][j] > threshold\n",
    "            nonzero_detections = detections.nonzero()[0]\n",
    "            if nonzero_detections.size > 0:\n",
    "                detection_time = (j*win_len)+nonzero_detections[0]/detections.size*0.5\n",
    "                detection_times.append(detection_time)\n",
    "                detection_predictions.append(np.max(predictions[i][j]))\n",
    "    return detection_times,detection_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ec93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Set some parameters\n",
    "\n",
    "'''\n",
    "\n",
    "# parameters of the preproccessed data saved as h5\n",
    "spacing = 1.017\n",
    "fs = 200\n",
    "chan_start = 331\n",
    "channels = [531,1361]\n",
    "num_chan = channels[1]-channels[0]\n",
    "chan_pos = (np.array(range(channels[0],channels[1]))-channels[0])*spacing\n",
    "\n",
    "# fk filtering parameters\n",
    "cmin = 3000\n",
    "cmax = np.inf\n",
    "sgn = \"pos\"\n",
    "\n",
    "# windowing parameters\n",
    "win_len = 0.5 # in seconds\n",
    "step_len = 0.1 # in seconds\n",
    "ns_input = 6000 # in samples: must be 6000!\n",
    "taper_len = 100\n",
    "\n",
    "# choose the model(s) to run\n",
    "models = ['original']#, 'ethz']#, 'instance', 'scedc', 'stead']#, 'neic']\n",
    "\n",
    "# choose a gpu\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# choose velocities to test\n",
    "vels = [4000,6000,8000]\n",
    "\n",
    "# number of samples to be thrown out on either side of each window of predictions\n",
    "blnd = [100, 100]\n",
    "\n",
    "# set output path\n",
    "out_path = \"/home/solinger/detections/eqt_funcs/\"\n",
    "\n",
    "# save every N files\n",
    "save_n = 1\n",
    "\n",
    "# set files\n",
    "path = \"/1-fnp/cascadia/c-wsd04/greenland/data/\"\n",
    "files = glob.glob(path+\"*\")\n",
    "files.sort()\n",
    "#fname = \"chans331-1360_fs200_2019-07-08T05:42:22-2019-07-08T05:42:51.h5\"\n",
    "#fname = \"chans331-1360_fs200_2019-07-07T00:09:22-2019-07-07T00:09:51.h5\"\n",
    "#files = [path+fname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc4633a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Run on the entire dataset\n",
    "\n",
    "'''\n",
    "\n",
    "# make output container\n",
    "predictions = []\n",
    "\n",
    "# start timer\n",
    "t = time.time()\n",
    "\n",
    "# initialize the models ahead of time\n",
    "eqts = []\n",
    "for i,model in enumerate(models): \n",
    "    eqts.append(sbm.EQTransformer.from_pretrained(model))\n",
    "    eqts[i].to(device)\n",
    "    eqts[i].eval()\n",
    "\n",
    "# iterate through each file\n",
    "for i,f in enumerate(files):\n",
    "    try:\n",
    "        print(f)\n",
    "        #read data\n",
    "        file = h5py.File(f)\n",
    "        data = file['data'][()]\n",
    "        file.close()\n",
    "\n",
    "        # subset to desired channels\n",
    "        data = data[:,channels[0]-chan_start:channels[1]-chan_start]\n",
    "\n",
    "        # fk filter(data,fs,dx,sgn='pos',cmin=5,cmax=50):\n",
    "        fk_data = fk(data,fs,spacing,cmin,cmax,sgn)\n",
    "\n",
    "        # set parameters relevant to upsampling\n",
    "        file_len = data.shape[0]/fs\n",
    "        fs_new = 6000/win_len\n",
    "        ns_new = int(file_len * fs_new)\n",
    "        times = np.arange(0,win_len,1/fs_new)\n",
    "\n",
    "        # perform upsampling\n",
    "        #tr = time.time()\n",
    "        up_data = resample(fk_data,ns_new,axis=0)\n",
    "        #print(\"Upsampled in\",time.time()-tr,\"s\")\n",
    "\n",
    "        # reshape the data\n",
    "        reshaped_data = np.reshape(up_data,(up_data.shape[0]//6000,6000,up_data.shape[1]))\n",
    "\n",
    "        # predict each window in the data\n",
    "        #tp = time.time()\n",
    "        pred = predict_each_window(reshaped_data,models,eqts,fs_new,channels,blnd,taper_len,vels)\n",
    "        predictions.append(pred)\n",
    "        #print(\"Predicted in\",time.time()-tp,\"s\")\n",
    "\n",
    "        # save output\n",
    "        if np.mod(i+1,save_n) == 0:\n",
    "\n",
    "            # save result for this file\n",
    "            t = time.time()\n",
    "            start_str = files[i+1-save_n].split(\"200_\")[1].split(\"-2019\")[0]\n",
    "            end_str = \"2019\" + files[i].split(\"200_\")[1].split(\"-2019\")[1].split(\".\")[0]\n",
    "            fname = (out_path+start_str+\"-\"+end_str+\".pickle\")\n",
    "\n",
    "            # save array of data to h5\n",
    "            with open(fname, \"wb\") as output_file:\n",
    "                pickle.dump(predictions, output_file)\n",
    "\n",
    "            # reset output container\n",
    "            predictions = []\n",
    "            print(\"saved in\",time.time()-t)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# stop timer\n",
    "timer = time.time() - t\n",
    "print(\"Did workflow in\",timer,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Investigate results for a particular file\n",
    "\n",
    "'''\n",
    "\n",
    "# choose a datetime\n",
    "datestring = \"2019-07-06T12:03:21\"\n",
    "\n",
    "# list the output files\n",
    "out_file = glob.glob(out_path+\"/\"+datestring+\"*\")[0]\n",
    "with open(out_file, \"rb\") as f:\n",
    "    predictions = pickle.load(f)\n",
    "\n",
    "# get some data\n",
    "file_ind = [datestring in f for f in files]\n",
    "data_file = np.array(files)[file_ind][0]\n",
    "file = h5py.File(data_file)\n",
    "data = file['data'][()]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Threshold all the predictions into detections\n",
    "\n",
    "'''\n",
    "\n",
    "# list the output files\n",
    "out_files = glob.glob(out_path+\"/*2019*\")\n",
    "    \n",
    "# set the threshold\n",
    "threshold = 50\n",
    "\n",
    "# trigger the detector\n",
    "all_detections = []\n",
    "all_detection_predictions = []\n",
    "for out_file in out_files:\n",
    "    with open(out_file, \"rb\") as f:\n",
    "        predictions = pickle.load(f)\n",
    "    detections,detection_predictions = trigger(predictions,threshold)\n",
    "    if len(detections) > 0:\n",
    "        fdate = datetime.datetime.strptime(out_file.split(\"funcs/\")[1].split(\"-2019\")[0],\"%Y-%m-%dT%H:%M:%S\")\n",
    "        detection_times = [fdate + datetime.timedelta(seconds=s) for s in detections]\n",
    "        all_detections.append(detection_times)\n",
    "        all_detection_predictions.append(detection_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe18d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Save the times of all triggered detections\n",
    "\n",
    "'''\n",
    "\n",
    "# save detection times\n",
    "det_file = out_path + \"detection_times.pickle\"\n",
    "with open(det_file, \"wb\") as output_file:\n",
    "    pickle.dump(all_detections, output_file)\n",
    "    \n",
    "# save summed prediction values for each detection time\n",
    "det_pred_file = out_path + \"detection_predictions.pickle\"\n",
    "with open(det_pred_file, \"wb\") as output_file:\n",
    "    pickle.dump(all_detection_predictions, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf65739",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot all the events\n",
    "\n",
    "'''\n",
    "\n",
    "# set some plotting parameters\n",
    "pre_buff = 0.2\n",
    "post_buff = 0.3\n",
    "\n",
    "# set output path\n",
    "fig_path = \"/home/solinger/detections/eqt_plots/\"\n",
    "\n",
    "# iterate through each detection\n",
    "for f in files:\n",
    "    \n",
    "    # get time lims of the file\n",
    "    file_start = datetime.datetime.strptime(f.split(\"200_\")[1].split(\"-2019\")[0],\"%Y-%m-%dT%H:%M:%S\")    \n",
    "    file_end = file_start + datetime.timedelta(seconds=29)\n",
    "\n",
    "    # find corresponding list of detections\n",
    "    detection_bool = np.where([det[0] > file_start and det[0] < file_end for det in all_detections])\n",
    "\n",
    "    # if there are detections in this file\n",
    "    if len(detection_bool[0]) > 0:\n",
    "        detection_ind = detection_bool[0][0] \n",
    "        detection_times = all_detections[detection_ind]\n",
    "        max_predictions = all_detection_predictions[detection_ind]\n",
    "        \n",
    "        # read the file\n",
    "        file = h5py.File(f)\n",
    "        data = file['data'][()]\n",
    "        file.close()\n",
    "\n",
    "        # plot each event\n",
    "        for i,d in enumerate(detection_times):\n",
    "\n",
    "            # get window start times\n",
    "            d_sec = d-file_start\n",
    "            win_start_sec = d_sec.total_seconds()-pre_buff\n",
    "            win_starttime = file_start + datetime.timedelta(seconds=win_start_sec)\n",
    "            win_end_sec = d_sec.total_seconds()+post_buff\n",
    "            win_endtime = file_start + datetime.timedelta(seconds=win_end_sec)\n",
    "            \n",
    "            # window the data\n",
    "            win_start = int(np.floor(fs*win_start_sec))\n",
    "            win_end = int(np.floor(fs*win_end_sec))\n",
    "            win_data = data[win_start:win_end,:]\n",
    "            t = pd.date_range(start=win_starttime,end=win_endtime,periods=win_data.shape[0])\n",
    "            \n",
    "            # make the plot\n",
    "            fig, ax = plt.subplots(figsize=(10,7))\n",
    "            ax.pcolormesh(t,np.array(range(0,channels[1]-331))*spacing,np.transpose(win_data),cmap='gray',vmin=-50,vmax=50)\n",
    "            plt.gca().invert_yaxis()\n",
    "            ax.set_ylabel(\"Depth (m)\")\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylim(channels[1]-331,0)\n",
    "            ax.vlines(d,ymin=331,ymax=channels[1],colors='r',linestyle=\"-\",alpha=0.35)\n",
    "            title = (\"Detection at \" + d.strftime(\"%Y-%m-%dT%H:%M:%S\") + \n",
    "                     \" ($\\Sigma P$ = \" + str(np.round(max_predictions[i],1))+ \")\")\n",
    "            ax.set_title(title)\n",
    "            \n",
    "            # save figure\n",
    "            fname = fig_path+d.strftime(\"%Y-%m-%dT%H:%M:%S\")+\"_P\"+str(np.round(max_predictions[i],1))+\".png\"\n",
    "            plt.savefig(fname)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c65acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Use STALTA on the event windows to determine (very) approximate depths\n",
    "\n",
    "'''\n",
    "\n",
    "# set some windowing parameters\n",
    "pre_buff = 0.4\n",
    "post_buff = 0.3\n",
    "\n",
    "# set stalta lengths (samples)\n",
    "sta = 5\n",
    "lta = 40\n",
    "\n",
    "# fk filtering parameters\n",
    "cmin = 3000\n",
    "cmax = np.inf\n",
    "sgn = \"pos\"\n",
    "\n",
    "# set threshold for image region size (minumum size to qualify as a moveout)\n",
    "size_thresh = 400\n",
    "\n",
    "# load detection times and predictions\n",
    "det_file = out_path + \"detection_times.pickle\"\n",
    "with open(det_file, \"rb\") as f:\n",
    "    all_detections = pickle.load(f)\n",
    "pred_file = out_path + \"detection_predictions.pickle\"\n",
    "with open(pred_file, \"rb\") as f:\n",
    "    all_predictions = pickle.load(f)\n",
    "\n",
    "# make dataframe for storing\n",
    "df = pd.DataFrame(columns=[\"detection time\",\"times\",\"first channel\", \"channels\"])\n",
    "\n",
    "# iterate through each detection\n",
    "for f in files:\n",
    "    \n",
    "    # get time lims of the file\n",
    "    file_start = datetime.datetime.strptime(f.split(\"200_\")[1].split(\"-2019\")[0],\"%Y-%m-%dT%H:%M:%S\")    \n",
    "    file_end = file_start + datetime.timedelta(seconds=29)\n",
    "\n",
    "    # find corresponding list of detections\n",
    "    detection_bool = np.where([det[0] > file_start and det[0] < file_end for det in all_detections])\n",
    "\n",
    "    # if there are detections in this file\n",
    "    if len(detection_bool[0]) > 0:\n",
    "        detection_ind = detection_bool[0][0] \n",
    "        detection_times = all_detections[detection_ind]\n",
    "        max_predictions = all_predictions[detection_ind]\n",
    "        \n",
    "        # read the file\n",
    "        file = h5py.File(f)\n",
    "        data = file['data'][()]\n",
    "        file.close()\n",
    "\n",
    "        # plot each event\n",
    "        for i,d in enumerate(detection_times):\n",
    "\n",
    "            # get window start times\n",
    "            d_sec = d-file_start\n",
    "            win_start_sec = d_sec.total_seconds()-pre_buff\n",
    "            win_starttime = file_start + datetime.timedelta(seconds=win_start_sec)\n",
    "            win_end_sec = d_sec.total_seconds()+post_buff\n",
    "            win_endtime = file_start + datetime.timedelta(seconds=win_end_sec)\n",
    "            \n",
    "            # window the data\n",
    "            win_start = int(np.floor(fs*win_start_sec))\n",
    "            win_end = int(np.floor(fs*win_end_sec))\n",
    "            win_data = data[win_start:win_end,:]\n",
    "            t = pd.date_range(start=win_starttime,end=win_endtime,periods=win_data.shape[0])\n",
    "\n",
    "            # fk filter the data\n",
    "            win_data = fk(win_data,fs,spacing,cmin,cmax,sgn)\n",
    "\n",
    "            # run stalta\n",
    "            argmax_im = np.zeros(win_data.shape)\n",
    "            for c in range(win_data.shape[1]):\n",
    "                stalta = recursive_sta_lta(win_data[:,c],sta,lta)\n",
    "                argmax_im[np.argmax(stalta),c] = 1\n",
    "\n",
    "            # dilate binary image to connect moveout regions\n",
    "            k = np.ones((2,15))\n",
    "            argmax_im = binary_dilation(argmax_im,structure=k)\n",
    "\n",
    "           # get rid of non-contiguous regions\n",
    "            id_regions, num_ids = label(argmax_im)\n",
    "            id_sizes = np.array(imsum(argmax_im, id_regions, range(num_ids + 1)))\n",
    "            area_mask = (id_sizes < size_thresh)\n",
    "            argmax_im[area_mask[id_regions]] = 0 \n",
    "\n",
    "            # get first arrival channel and time of each moveout\n",
    "            id_regions, num_ids = label(argmax_im)\n",
    "            for i in range(1,num_ids+1):\n",
    "                region_im = copy.deepcopy(argmax_im)\n",
    "                region_im[np.array(id_regions != i)] = 0\n",
    "                moveout_channels = 331+np.where(region_im)[1]\n",
    "                moveout_times = t[np.where(region_im)[0]]\n",
    "                out_data = [[moveout_times[0],moveout_times,moveout_channels[0],moveout_channels]]\n",
    "                temp_df = pd.DataFrame(out_data,columns=[\"detection time\",\"times\",\"first channel\", \"channels\"])\n",
    "                df = pd.concat([df,temp_df],ignore_index=True)\n",
    "                \n",
    "# save the resulting catalog\n",
    "df.to_pickle(\"/home/solinger/detections/processed_eqt_detections.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70dcba73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m filtered_detection_times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(filtered_detection_times)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# take difference in event times\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m event_spacing \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_detection_times\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m event_spacing \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([e\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m event_spacing])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# remove 0 spacing values (double counted events)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/elep/lib/python3.11/site-packages/numpy/lib/function_base.py:1452\u001b[0m, in \u001b[0;36mdiff\u001b[0;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[1;32m   1450\u001b[0m op \u001b[38;5;241m=\u001b[39m not_equal \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_ \u001b[38;5;28;01melse\u001b[39;00m subtract\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m-> 1452\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslice1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslice2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m~/anaconda3/envs/elep/lib/python3.11/site-packages/pandas/_libs/tslibs/timestamps.pyx:536\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps._Timestamp.__sub__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/elep/lib/python3.11/site-packages/pandas/_libs/tslibs/timedeltas.pyx:1587\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timedeltas._Timedelta._from_value_and_reso\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/elep/lib/python3.11/site-packages/pandas/_libs/tslibs/timedeltas.pyx:954\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timedeltas._timedelta_from_value_and_reso\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Investigate event timing on a quiet day\n",
    "\n",
    "'''\n",
    "\n",
    "# choose time window of interest\n",
    "starttime = datetime.datetime(2019,7,5)\n",
    "endtime = datetime.datetime(2019,7,9)\n",
    "\n",
    "# set depth bounds \n",
    "spacing = 1.017\n",
    "depth_upper = 0\n",
    "depth_lower = 1200\n",
    "\n",
    "# load detection times and predictions\n",
    "det_file = \"/home/solinger/detections/processed_eqt_detections.pickle\"\n",
    "with open(det_file, \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "# get icequake apparent depths and detection times\n",
    "depths = (np.array(df[\"first channel\"].to_list())-331)*spacing\n",
    "detection_times = np.array(df[\"detection time\"].to_list())\n",
    "        \n",
    "# filter by apparent depth\n",
    "filtered_detection_times = []\n",
    "for i,d in enumerate(detection_times):\n",
    "    if depths[i] < depth_lower and depths[i] > depth_upper:\n",
    "        filtered_detection_times.append(d)\n",
    "filtered_detection_times = np.array(filtered_detection_times)\n",
    "    \n",
    "# take difference in event times\n",
    "event_spacing = np.diff(filtered_detection_times)\n",
    "event_spacing = np.array([e.total_seconds() for e in event_spacing])\n",
    "\n",
    "# remove 0 spacing values (double counted events)\n",
    "spacing_times = detection_times[1:][event_spacing != 0]\n",
    "event_spacing = event_spacing[event_spacing != 0]\n",
    "\n",
    "# apply a moving average filter\n",
    "mean_win_size = 10\n",
    "smoothed_spacing = sliding_window_view(event_spacing, mean_win_size).mean(axis=-1)\n",
    "detections_datenum = [d.to_pydatetime().timestamp() for d in spacing_times]\n",
    "smoothed_detections_datenum = sliding_window_view(detections_datenum, mean_win_size).mean(axis=-1)\n",
    "smoothed_detections = [datetime.datetime.fromtimestamp(d) for d in smoothed_detections_datenum]\n",
    "\n",
    "# get freq for both\n",
    "freq = [1/e for e in event_spacing]\n",
    "smoothed_freq = [1/e for e in smoothed_spacing]\n",
    "\n",
    "# make a basic plot\n",
    "fig,ax = plt.subplots(figsize=(15,7))\n",
    "ax.scatter(spacing_times,freq,c='black')\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "# make a basic plot\n",
    "ax.plot(smoothed_detections,smoothed_freq,c='orangered',linewidth=3)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d2757f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'depths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdepths\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'depths' is not defined"
     ]
    }
   ],
   "source": [
    "depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f70d9d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Timestamp('2019-07-05 13:12:46.643580697'),\n",
       "       Timestamp('2019-07-05 13:13:09.595187899'),\n",
       "       Timestamp('2019-07-05 13:13:09.705979266'), ...,\n",
       "       Timestamp('2019-07-08 18:35:05.940539374'),\n",
       "       Timestamp('2019-07-08 18:35:16.350114381'),\n",
       "       Timestamp('2019-07-08 18:35:16.506229489')], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacing_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c1c63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.56233256e+09, 1.56233259e+09, 1.56233263e+09, ...,\n",
       "       1.56261089e+09, 1.56261090e+09, 1.56261090e+09])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_detections_datenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb5b3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Visualize the catalog\n",
    "\n",
    "'''\n",
    "\n",
    "# get icequake apparent depths and detection times\n",
    "depths = (np.array(df[\"first channel\"].to_list())-331)*spacing\n",
    "detection_times = df[\"detection time\"].to_list()\n",
    "\n",
    "# plot depth and time\n",
    "fig,ax = plt.subplots(figsize=(15,7))\n",
    "ax.grid(True)\n",
    "plt.scatter(df[\"detection time\"].to_list(),depths,s=4,c=depths)\n",
    "plt.plasma()\n",
    "plt.gca().invert_yaxis()\n",
    "ax.set_ylabel(\"Depth (m)\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylim(channels[1]-331,0)\n",
    "ax.set_xlim(detection_times[0],detection_times[-1])\n",
    "plt.show()\n",
    "\n",
    "# zoomed plot depth and time\n",
    "fig,ax = plt.subplots(figsize=(15,7))\n",
    "ax.grid(True)\n",
    "plt.scatter(detection_times,depths,s=10,c=depths)\n",
    "plt.plasma()\n",
    "plt.gca().invert_yaxis()\n",
    "ax.set_ylabel(\"Depth (m)\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylim(channels[1]-331,0)\n",
    "start_stamp = datetime.datetime(2019,7,8,5)\n",
    "end_stamp = datetime.datetime(2019,7,8,6,15)\n",
    "ax.set_xlim(start_stamp,end_stamp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_file = \"/home/solinger/detections/processed_eqt_detections.pickle\"\n",
    "with open(det_file, \"rb\") as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ef321",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca6788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145702c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elep",
   "language": "python",
   "name": "elep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
