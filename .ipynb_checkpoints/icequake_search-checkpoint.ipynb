{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e845979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "from obspy.signal.trigger import classic_sta_lta, recursive_sta_lta, coincidence_trigger\n",
    "from obspy_local.obspy_local.io.segy.core import _read_segy\n",
    "import h5py\n",
    "from scipy.signal import spectrogram, butter, sosfilt, decimate, detrend\n",
    "from scipy.signal.windows import hann\n",
    "import scipy\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib. pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as datetime\n",
    "import pickle\n",
    "import collections\n",
    "import multiprocessing\n",
    "import types\n",
    "import copy\n",
    "from detection.stalta import stalta_detector\n",
    "from detection.template_matching import template_match\n",
    "from detection.waveforms import plot_moveout, save_dataset_h5\n",
    "import struct\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeafb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot the entire dataset on a few a different frequency bands at a few channels\n",
    "\n",
    "'''\n",
    "\n",
    "# choose which channels to plot\n",
    "channels = [331,431,531,631,731,831,931,1031]\n",
    "\n",
    "# choose time limits (we want to exclude all the anthropoenic noise near the start)\n",
    "starttime = obspy.UTCDateTime(2019,7,6,12)\n",
    "endtime = obspy.UTCDateTime(2019,7,9)\n",
    "\n",
    "# set frequency bins\n",
    "freq_bins = [0.001,0.01,0.1,1,10,100,500]\n",
    "\n",
    "# get list of all continuous channel files\n",
    "path = \"/1-fnp/pnwstore1/p-wd05/greenland/data/channel/\"\n",
    "files = []\n",
    "for channel in channels:\n",
    "    files.append(path+\"channel_\"+str(channel)+\".mseed\")\n",
    "\n",
    "# iterate through input files\n",
    "for f in files:\n",
    "    \n",
    "    # read and trim data\n",
    "    st = obspy.read(f)\n",
    "    st.trim(starttime=starttime,endtime=endtime)\n",
    "    \n",
    "    # filter on a few bands and plot\n",
    "    for b in range(len(freq_bins)-1):\n",
    "\n",
    "        # filter the data\n",
    "        st_filt = st.copy()\n",
    "        st_filt.filter(\"bandpass\",freqmin=freq_bins[b],freqmax=freq_bins[b+1])\n",
    "        \n",
    "        # semi-intelligently auto-limit the y axis by fin\n",
    "        distance = 86400/2*1000\n",
    "        peak_indices = scipy.signal.find_peaks(st_filt[0].data,distance=distance)[0]\n",
    "        peaks = st_filt[0].data[peak_indices]\n",
    "        ymax = np.partition(peaks.flatten(), -3)[-3]\n",
    "\n",
    "        # make a plot\n",
    "        channel = f.split(\"_\")[1].split(\".\")[0]\n",
    "        fname = \"/home/solinger/continuous/channel_\"+channel+\"_\"+str(freq_bins[b])+\"-\"+str(freq_bins[b+1])+\"_Hz.png\"\n",
    "        fig = st_filt[0].plot(dpi=100,qdp=\"off\",size=(1500,600),handle=True)\n",
    "        ax = fig.axes[0]\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d \\n %H:%M'))\n",
    "        ax.set_xticks([18083.5, 18083.75, 18084. ,  18084.25, 18084.5, 18084.75,  18085. , 18085.25, 18085.5])\n",
    "        ax.set_ylim([-ymax,ymax])\n",
    "        plt.title(\"Channel \"+channel+\" (\"+str(freq_bins[b])+\"-\"+str(freq_bins[b+1])+\" Hz)\")\n",
    "        plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbde1eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Investigate the icequakes at 10-100Hz\n",
    "\n",
    "'''\n",
    "\n",
    "# choose which channels to plot\n",
    "channels = [331,431,531,631,731,831,931,1031]\n",
    "\n",
    "# set time limits\n",
    "# starttime = obspy.UTCDateTime(2019,7,8,4,45)\n",
    "# endtime = obspy.UTCDateTime(2019,7,8,6,30)\n",
    "starttime = obspy.UTCDateTime(2019,7,8,5,42,35)\n",
    "endtime = obspy.UTCDateTime(2019,7,8,5,43,5)\n",
    "\n",
    "# set frequency band\n",
    "freq = [10,100]\n",
    "\n",
    "# get list of all continuous channel files\n",
    "path = \"/1-fnp/pnwstore1/p-wd05/greenland/data/channel/\"\n",
    "files = []\n",
    "for channel in channels:\n",
    "    files.append(path+\"channel_\"+str(channel)+\".mseed\")\n",
    "\n",
    "# iterate through input files\n",
    "for f in files:\n",
    "    \n",
    "    # read, trim and filter data\n",
    "    st = obspy.read(f)\n",
    "    st.trim(starttime=starttime,endtime=endtime)\n",
    "    st_filt = st.copy()\n",
    "    st_filt.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "\n",
    "    # semi-intelligently auto-limit the y axis by fin\n",
    "    distance = 3600*1000\n",
    "    peak_indices = scipy.signal.find_peaks(st_filt[0].data,distance=distance)[0]\n",
    "    peaks = st_filt[0].data[peak_indices]\n",
    "    ymax = np.partition(peaks.flatten(), -3)[-3]\n",
    "\n",
    "    # make a plot\n",
    "    channel = f.split(\"_\")[1].split(\".\")[0]\n",
    "    fname = \"/home/solinger/continuous/channel_\"+channel+\"_\"+str(freq[0])+\"-\"+str(freq[1])+\"_Hz_zoom.png\"\n",
    "    fig = st_filt[0].plot(dpi=100,qdp=\"off\",size=(1500,600),handle=True)\n",
    "    ax = fig.axes[0]\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d \\n %H:%M:%S'))\n",
    "    #ax.set_ylim([-ymax,ymax])\n",
    "    plt.title(\"Channel \"+channel+\" (\"+str(freq[0])+\"-\"+str(freq[1])+\" Hz)\")\n",
    "    plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Find the 30s file that contains the above\n",
    "\n",
    "'''\n",
    "\n",
    "# choose frequency band\n",
    "freq = [20,100]\n",
    "\n",
    "# list all the natively 1khz files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "\n",
    "# list all the resamopled 1khz files\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "\n",
    "# combine and sort\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort()\n",
    "\n",
    "# calculate (approximately) which file it should be\n",
    "win_start = obspy.UTCDateTime(2019,7,8,5,42,35)\n",
    "files_start = _read_segy(files_1khz_all[1])[0].stats.starttime\n",
    "file_number = round((win_start - files_start) / 30)\n",
    "print(\"Somewhere around...\"+str(file_number))\n",
    "\n",
    "# read the relevant files, filter, and combine\n",
    "st1 = _read_segy(files_1khz_all[7346])\n",
    "st2 = _read_segy(files_1khz_all[7347])\n",
    "st1.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "st2.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "\n",
    "# add some helpful metadata\n",
    "for s in [st1,st2]:\n",
    "    for i in range(len(s)):\n",
    "        s[i].stats.station = str(i)\n",
    "st = st1 + st2\n",
    "\n",
    "# merge traces (note that this reorders them based on station code, so you MUST use st.select(), not naive indexing)\n",
    "st.merge()\n",
    "\n",
    "# plot to verify we're in the right time period\n",
    "st.select(station=\"1031\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Get moveout data for the entire window\n",
    "\n",
    "'''\n",
    "\n",
    "# select channels of interest\n",
    "channels = range(331,1361)\n",
    "\n",
    "# make container\n",
    "npts = st[0].stats.npts\n",
    "data = np.zeros((npts,len(channels)))\n",
    "\n",
    "# fill container with data from each desired channel\n",
    "for channel in channels:\n",
    "    data[:,channel-channels[0]] = st.select(station=str(channel))[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910bee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Get moveout data for a single event\n",
    "\n",
    "'''\n",
    "\n",
    "# set time limits and trim\n",
    "starttime = obspy.UTCDateTime(2019,7,8,5,42,41)\n",
    "endtime = obspy.UTCDateTime(2019,7,8,5,42,42)\n",
    "st_event = st.copy().trim(starttime=starttime,endtime=endtime)\n",
    "\n",
    "# select channels of interest\n",
    "channels = range(331,1361)\n",
    "\n",
    "# make container\n",
    "npts = st_event[0].stats.npts\n",
    "event_data = np.zeros((npts,len(channels)))\n",
    "\n",
    "# fill container with data from each desired channel\n",
    "for channel in channels:\n",
    "    event_data[:,channel-channels[0]] = st_event.select(station=str(channel))[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11830249",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot the moveouts\n",
    "\n",
    "'''\n",
    "\n",
    "plot_moveout(data,st,channels)\n",
    "ax = plot_moveout(event_data,st_event,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ae825",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Run STALTA detections\n",
    "\n",
    "'''\n",
    "\n",
    "# make detection parameter object\n",
    "d = types.SimpleNamespace()\n",
    "    \n",
    "# set detection parameters\n",
    "d.channels = range(331,1361)\n",
    "d.sta = 0.05\n",
    "d.lta = 3\n",
    "d.thr_on = 7\n",
    "d.thr_off = 1\n",
    "d.thr_coincidence_sum = 400\n",
    "d.trigger_off_extension = 1000\n",
    "d.freq = [10,100]\n",
    "\n",
    "# list all the files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort() \n",
    "d.files = files_1khz_all\n",
    "\n",
    "# choose output direction\n",
    "d.out_path = \"/home/solinger/icequakes/detections/\"\n",
    "\n",
    "# set the duration in seconds of each batch of data to run detections on\n",
    "d.batch_length = 300\n",
    "d.file_length = 30\n",
    "\n",
    "# choose number of processors\n",
    "d.n_procs = 24\n",
    "\n",
    "# run detections\n",
    "stalta_detector(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Visualize catalog\n",
    "\n",
    "'''\n",
    "\n",
    "# set an additional coincidence threshold\n",
    "thr_coincidence_sum = 0\n",
    "\n",
    "# set a time window \n",
    "win_start = datetime.datetime(2019,7,5)\n",
    "win_end = datetime.datetime(2019,7,9)\n",
    "\n",
    "# list detections for each 30s file\n",
    "detection_path = \"/home/solinger/icequakes/detections/stalta/*\"\n",
    "detection_files = glob.glob(detection_path)\n",
    "\n",
    "# read all files\n",
    "all_triggers = []\n",
    "for f in detection_files:\n",
    "    file = open(f,'rb')\n",
    "    triggers = pickle.load(file)\n",
    "    file.close()\n",
    "    all_triggers.extend(triggers)\n",
    "    \n",
    "# get depth for each trigger\n",
    "spacing = 1.017\n",
    "start_chan = 331\n",
    "depths = []\n",
    "trigger_times = []\n",
    "triggers = []\n",
    "for trigger in all_triggers:\n",
    "    if trigger['coincidence_sum'] > thr_coincidence_sum:\n",
    "        depth = spacing*(int(trigger['stations'][0])-start_chan)\n",
    "        trigger_time = trigger['time'].datetime\n",
    "        if trigger_time > win_start and trigger_time < win_end:\n",
    "            depths.append(depth)\n",
    "            trigger_times.append(trigger_time)\n",
    "            triggers.append(trigger)\n",
    "\n",
    "# plot catalog depths as a function of time\n",
    "fig,ax = plt.subplots(figsize=(15,10))\n",
    "plt.scatter(trigger_times,depths)\n",
    "plt.gca().invert_yaxis()\n",
    "ax.set_ylabel(\"Depth (m)\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d \\n %H:%M:%S'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Plot moveout for some events\n",
    "\n",
    "'''\n",
    "\n",
    "# choose frequency band\n",
    "freq = [20,100]\n",
    "\n",
    "# list all the natively 1khz files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "\n",
    "# list all the resamopled 1khz files\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "\n",
    "# combine and sort\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort()\n",
    "\n",
    "# set a time window \n",
    "win_start = datetime.datetime(2019,7,8,5,42)\n",
    "#win_start = datetime.datetime(2019,7,6,7,42)\n",
    "\n",
    "# get file closest to above\n",
    "file_selection = []\n",
    "for f in files_1khz_all:\n",
    "    if win_start.strftime(\"%y%m%d%H%M\") in f:\n",
    "        file_selection.append(f)\n",
    "\n",
    "# read the relevant files, filter, and combine\n",
    "st = _read_segy(file_selection[0])\n",
    "st.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "\n",
    "# add some helpful metadata\n",
    "for i in range(len(st)):\n",
    "    st[i].stats.station = str(i)\n",
    "\n",
    "# extract data from each channel to visualize moveout\n",
    "event_start = obspy.UTCDateTime(2019,7,8,5,42,50)\n",
    "event_end = obspy.UTCDateTime(2019,7,8,5,42,54)\n",
    "event_end = st[0].stats.endtime\n",
    "st_event = st.copy()\n",
    "st_event.trim(starttime=event_start,endtime=event_end)\n",
    "\n",
    "# plot to verify we're in the right time period\n",
    "st_event.select(station=\"1031\").plot();\n",
    "\n",
    "# select channels of interest\n",
    "channels = range(331,1361)\n",
    "#channels = range(1060,1100)\n",
    "\n",
    "# make container\n",
    "npts = st_event[0].stats.npts\n",
    "event_data = np.zeros((npts,len(channels)))\n",
    "\n",
    "# fill container with data from each desired channel\n",
    "for channel in channels:\n",
    "    event_data[:,channel-channels[0]] = st_event.select(station=str(channel))[0].data\n",
    "\n",
    "# plot the moveout\n",
    "ax = plot_moveout(event_data,st_event,channels)\n",
    "\n",
    "# run detections for just this file\n",
    "# triggers = stalta_coincidence(sta,lta,thr_on,thr_off,thr_coincidence_sum,trigger_off_extension,\n",
    "#                    [file_selection[0]],channels,freq)\n",
    "# trigger_times = trigger_times = [t['time'] for t in triggers]\n",
    "\n",
    "# plot detections\n",
    "utc_trigger_times = np.array([obspy.UTCDateTime(t) for t in trigger_times])\n",
    "window_idx = [(utc_trigger_times > event_start) & (utc_trigger_times < event_end)][0]\n",
    "window_trigger_times = np.array(trigger_times)[window_idx]\n",
    "ax.vlines(window_trigger_times,ymin=0,ymax=spacing*(channels[-1]-channels[0]),colors=\"r\",linestyles=\"--\")\n",
    "ax.set_title(st_event[0].stats.starttime.strftime(\"%Y-%m-%dT%H:%M:%S\") \n",
    "            + \" - \" + st_event[0].stats.endtime.strftime(\"%Y-%m-%dT%H:%M:%S\")+\" Moveout\")\n",
    "plt.savefig(\"/home/solinger/icequakes/moveouts/\" + st_event[0].stats.starttime.strftime(\"%Y-%m-%dT%H:%M:%S\") \n",
    "            + \"-\" + st_event[0].stats.endtime.strftime(\"%Y-%m-%dT%H:%M:%S\")+\"_moveout.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Plot moveout for some events\n",
    "\n",
    "'''\n",
    "\n",
    "# choose frequency band\n",
    "freq = [20,100]\n",
    "\n",
    "# list all the natively 1khz files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "\n",
    "# list all the resamopled 1khz files\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "\n",
    "# combine and sort\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort()\n",
    "\n",
    "# set a time window \n",
    "win_start = datetime.datetime(2019,7,8,5,42)\n",
    "#win_start = datetime.datetime(2019,7,6,7,42)\n",
    "\n",
    "# get file closest to above\n",
    "file_selection = []\n",
    "for f in files_1khz_all:\n",
    "    if win_start.strftime(\"%y%m%d%H%M\") in f:\n",
    "        file_selection.append(f)\n",
    "\n",
    "# read the relevant files, filter, and combine\n",
    "st = _read_segy(file_selection[0])\n",
    "st.filter(\"bandpass\",freqmin=freq[0],freqmax=freq[1])\n",
    "\n",
    "# add some helpful metadata\n",
    "for i in range(len(st)):\n",
    "    st[i].stats.station = str(i)\n",
    "\n",
    "# plot to verify we're in the right time period\n",
    "st.select(station=\"1031\").plot();\n",
    "\n",
    "# select channels of interest\n",
    "channels = range(331,1361)\n",
    "#channels = range(1060,1100)\n",
    "\n",
    "# make container\n",
    "npts = st[0].stats.npts\n",
    "window_data = np.zeros((npts,len(channels)))\n",
    "\n",
    "# fill container with data from each desired channel\n",
    "for channel in channels:\n",
    "    window_data[:,channel-channels[0]] = st.select(station=str(channel))[0].data\n",
    "\n",
    "# plot the moveout\n",
    "ax = plot_moveout(window_data,st,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd8e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Save the entire preprocessed dataset as h5 files\n",
    "\n",
    "FIGURE OUT WHY THIS DOESNT EXECUTE PROPERLY WHEN DEFINED IN MODULE!\n",
    "RUNS BUT DOESN'T ERROR WHEN TRYING TO READ BAD FILE WHEN...\n",
    "    1. EVERYTHING IN MODULE WITHOUT __name__ ==__'main'__ \n",
    "    2. FUNCTIONS DEFINED IN MODULE WITH imap_unordered IN __name__ ==__'main'__ IN NOTEBOOK\n",
    "RUNS PROPERLY WHEN EVERYTHING IS DEFINED IN NOTEBOOK WITH imap_unordered IN __name__ ==__'main'__\n",
    "\n",
    "'''\n",
    "\n",
    "# list all the files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort() \n",
    "\n",
    "# set save file saving parameters\n",
    "s = types.SimpleNamespace()\n",
    "s.channels = range(331,1361)\n",
    "s.file_length = 30\n",
    "s.duration = 300\n",
    "s.fs = 1000\n",
    "s.freq = [10,100]\n",
    "s.taper_length = 1\n",
    "s.out_path = \"/store_ssd4/greenland/data/\"\n",
    "s.files = files_1khz_all[1:3]\n",
    "s.n_procs = 24\n",
    "\n",
    "# save the dataset as h5 files\n",
    "save_dataset_h5(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a46676",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Define some templates\n",
    "\n",
    "'''\n",
    "\n",
    "# set parameters\n",
    "fs = 200\n",
    "template_length = 1\n",
    "templates = []\n",
    "\n",
    "# read first file of data containing templates\n",
    "path = \"/store_ssd4/greenland/data/\"\n",
    "fname = \"chans331-1360_fs200_2019-07-08T05:42:22-2019-07-08T05:42:51.h5\"\n",
    "file = h5py.File(path+fname)\n",
    "data = file['data'][()]\n",
    "file.close()\n",
    "\n",
    "# add to list of templates\n",
    "templates.append(data[3825:3825+template_length*fs,:])\n",
    "templates.append(data[4725:4725+template_length*fs,:])\n",
    "\n",
    "# read first file of data containing templates\n",
    "fname = \"chans331-1360_fs200_2019-07-08T05:42:52-2019-07-08T05:43:21.h5\"\n",
    "file = h5py.File(path+fname)\n",
    "data = file['data'][()]\n",
    "file.close()\n",
    "\n",
    "# add to list of templates\n",
    "templates.append(data[380:380+template_length*fs,:])\n",
    "templates.append(data[1110:1110+template_length*fs,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29715833",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Run template matching detections\n",
    "\n",
    "'''\n",
    "\n",
    "# make correlation parameter object\n",
    "c = types.SimpleNamespace()\n",
    "\n",
    "# list all the files\n",
    "path = \"/store_ssd4/greenland/data/*\"\n",
    "files = glob.glob(path)\n",
    "files.sort() \n",
    "c.files = files\n",
    "\n",
    "# choose output direction\n",
    "c.out_path = \"/store_ssd4/greenland/template_correlations/\"\n",
    "\n",
    "# add template defined above\n",
    "c.templates = templates\n",
    "\n",
    "# choose number of processors\n",
    "c.n_procs = 24\n",
    "\n",
    "# run the basic correlation code\n",
    "corr = template_match(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e9cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_coincidence(corr,t,d):\n",
    "    max_corr = np.max(np.abs(corr),axis=0)\n",
    "    num_triggers = np.sum(max_corr > d.corr_threshold)\n",
    "    if num_triggers > d.num_triggers_threshold:\n",
    "        d.triggers.append(t)\n",
    "    return d\n",
    "\n",
    "\n",
    "def sample_coincidence(corr,t,d)\n",
    "    max_corr = np.max(np.abs(corr),axis=0)\n",
    "    num_triggers = np.sum(max_corr > d.corr_threshold)\n",
    "    if num_triggers > d.num_triggers_threshold:\n",
    "        d.triggers.append(t)\n",
    "    return d\n",
    "\n",
    "\n",
    "def trigger(corr,t,d):\n",
    "    num_windows = corr.shape[0]\n",
    "    d.triggers = []\n",
    "    for w in range(num_windows):\n",
    "        if d.mode == \"window\":\n",
    "            d = window_coincidence(corr[w,:,:],t[w],d)\n",
    "        elif d.mode == \"sample\"\n",
    "            d = sample_coincidence(corr[w,:,:],t[w],d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def detect(d):\n",
    "    \n",
    "    # make output container\n",
    "    detections = {}\n",
    "    for i in range(d.num_templates):\n",
    "         detections['template_'+str(i)+\"_detections\"] = []\n",
    "\n",
    "    # read the file\n",
    "    try:\n",
    "        with h5py.File(d.file,\"r\") as f:\n",
    "\n",
    "            # get time axis\n",
    "            startstr = d.file.split(\"/\")[-1].split(\"_\")[0].split(\"-2019\")[0]\n",
    "            endstr = \"2019\"+d.file.split(\"/\")[-1].split(\"_\")[0].split(\"-2019\")[-1]\n",
    "            starttime = datetime.datetime.strptime(startstr,\"%Y-%m-%dT%H:%M:%S\")\n",
    "            endtime = datetime.datetime.strptime(endstr,\"%Y-%m-%dT%H:%M:%S\")+datetime.timedelta(seconds=1)\n",
    "            datetimes = np.arange(starttime,endtime,datetime.timedelta(seconds=1)).astype(datetime.datetime)\n",
    "\n",
    "            # get correlation functions for each template\n",
    "            for i in range(d.num_templates):\n",
    "                key = 'template_'+str(i)+\"_corr\"\n",
    "                corr = f[key][()]\n",
    "                d = trigger(corr,datetimes,d)\n",
    "                detections['template_'+str(i)+\"_detections\"].extend(d.triggers)\n",
    "        f.close()\n",
    "    except:\n",
    "        print(\"Issue with file \" + str(d.file))\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ec078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with file /store_ssd4/greenland/template_correlations/2019-07-05T13:23:05-2019-07-05T13:23:34_corr.h5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Run triggering algorithm on template correlations\n",
    "\n",
    "'''\n",
    "\n",
    "# define detection object\n",
    "d = types.SimpleNamespace()\n",
    "\n",
    "# list all correlation files\n",
    "path = \"/store_ssd4/greenland/template_correlations/\"\n",
    "d.files = glob.glob(path+\"/*\")\n",
    "\n",
    "# set detection / triggering parameters\n",
    "d.num_templates = 4\n",
    "d.corr_threshold = 0.45\n",
    "d.num_triggers_threshold = 15\n",
    "d.n_procs = 24\n",
    "d.fs = 200\n",
    "d.mode = \"sample\"\n",
    "d.\n",
    "\n",
    "# make output container\n",
    "detections = {}\n",
    "for i in range(d.num_templates):\n",
    "     detections['template_'+str(i)+\"_detections\"] = []\n",
    "\n",
    "# make iterable list of inputs\n",
    "inputs = []\n",
    "for f in d.files:\n",
    "    d.file = f\n",
    "    inputs.append(copy.deepcopy(d))\n",
    "        \n",
    "# map inputs to detect and append as each call finishes\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    multiprocessing.freeze_support()\n",
    "    p = multiprocessing.Pool(processes=d.n_procs)\n",
    "    for result in p.imap_unordered(detect,inputs):\n",
    "        for i in range(d.num_templates):\n",
    "            detections['template_'+str(i)+\"_detections\"].extend(result['template_'+str(i)+\"_detections\"])\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c98b849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1895305/2814793466.py:56: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  ax.pcolormesh(range(event_data.shape[0]),(np.array(channels)-channels[0])*spacing,np.transpose(event_data),cmap='gray')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Analyze the detection results\n",
    "\n",
    "'''\n",
    "\n",
    "# consolidate results from each template\n",
    "all_detections = []\n",
    "for i in range(len(detections)):\n",
    "    all_detections.extend(detections['template_'+str(i)+\"_detections\"])\n",
    "all_detections = np.unique(all_detections)\n",
    "\n",
    "# list all data files\n",
    "path = \"/store_ssd4/greenland/data/\"\n",
    "files = glob.glob(path+\"*\")\n",
    "\n",
    "# set some data parameters\n",
    "fs = 200\n",
    "spacing = 1.017\n",
    "channels = range(331,1361)\n",
    "pre_buffer = 0.5\n",
    "event_duration = 1\n",
    "\n",
    "# get starttime of each file\n",
    "starttimes = []\n",
    "for f in files:\n",
    "    startstr = f.split(\"fs200_\")[-1].split(\"_\")[0].split(\"-2019\")[0]\n",
    "    starttimes.append(datetime.datetime.strptime(startstr,\"%Y-%m-%dT%H:%M:%S\"))\n",
    "starttimes = np.array(starttimes)\n",
    "\n",
    "#fname = \"chans331-1360_fs200_2019-07-08T05:42:22-2019-07-08T05:42:51.h5\"\n",
    "for detection in all_detections[331:]:\n",
    "    \n",
    "    # figure out which file to read\n",
    "    time_diffs = detection-np.array(starttimes)\n",
    "    file_idx = [(time_diffs > datetime.timedelta(seconds=0)) & (time_diffs <= datetime.timedelta(seconds=30))][0]\n",
    "    file_to_read = np.array(files)[file_idx][0]\n",
    "    event_start = time_diffs[file_idx][0].seconds\n",
    "    \n",
    "    # read the file\n",
    "    with h5py.File(file_to_read,\"r\") as f:\n",
    "        \n",
    "        # get data\n",
    "        data = f['data'][()]\n",
    "        start_idx = int((event_start-pre_buffer)*fs)\n",
    "        end_idx = int(start_idx + (pre_buffer+event_duration)*fs)\n",
    "        event_data = data[start_idx:end_idx,:]\n",
    "        \n",
    "        # get timing info\n",
    "        starttime = np.array(starttimes)[file_idx][0]\n",
    "        \n",
    "        # make the plot\n",
    "        fig,ax = plt.subplots(figsize=(15,10))\n",
    "        timestring = detection.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        ax.set_title(timestring)\n",
    "        ax.pcolormesh(range(event_data.shape[0]),(np.array(channels)-channels[0])*spacing,np.transpose(event_data),cmap='gray')\n",
    "        ticks = np.arange(0,event_data.shape[0],100)\n",
    "        ax.set_xticks(ticks)\n",
    "        ticklabels = ticks/fs\n",
    "        ax.set_xticklabels(ticklabels)\n",
    "        plt.gca().invert_yaxis()\n",
    "        ax.set_ylabel(\"Depth (m)\")\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        plt.savefig(\"/home/solinger/icequakes/detections/template_matching/plots/\"+timestring+\".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86004e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
