{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b3afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy_local\n",
    "import obspy\n",
    "from obspy_local.obspy_local.io.segy.core import _read_segy\n",
    "import glob\n",
    "from scipy.signal import spectrogram, butter, sosfilt, decimate, detrend\n",
    "from scipy.signal.windows import hann\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import os\n",
    "import types\n",
    "import copy\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbd4af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Read and downsample the VSP files\n",
    "\n",
    "'''\n",
    "\n",
    "f = \"borehole_data/VSP/RESPONDER_Jul2019_L028_300mSouth.sgy\"\n",
    "st = _read_segy(f,npts=2799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a7c9e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046 Trace(s) in Stream:\n",
       "\n",
       "Seq. No. in line:  318 | 2019-07-07T12:14:03.000000Z - 2019-07-07T12:14:03.699500Z | 4000.0 Hz, 2799 samples\n",
       "...\n",
       "(1044 other traces)\n",
       "...\n",
       "Seq. No. in line: 1363 | 2019-07-07T12:14:03.000000Z - 2019-07-07T12:14:03.699500Z | 4000.0 Hz, 2799 samples\n",
       "\n",
       "[Use \"print(Stream.__str__(extended=True))\" to print all Traces]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for converting files\n",
    "def downsample_file(f, out_path, nx_old, nx_new, fs_new):\n",
    "    try:\n",
    "        st = _read_segy(f,npts=nx_old)\n",
    "\n",
    "        # resample\n",
    "        st = st.resample(fs_new)\n",
    "\n",
    "        # correct metadata\n",
    "        st.stats.binary_file_header.number_of_samples_per_data_trace = nx_new\n",
    "        st.stats.binary_file_header.sample_interval_in_microseconds = int(1/fs_new*1e6)\n",
    "\n",
    "        # convert to float32\n",
    "        for tr in st:\n",
    "            tr.data = tr.data.astype('float32')\n",
    "\n",
    "        # write file\n",
    "        st.write(out_path+f.split(\"/\")[-1],format=\"SEGY\")\n",
    "    except:\n",
    "        print(\"Issue processing file \" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the 4khz Greenland files\n",
    "path_4khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/\"\n",
    "files_4khz = glob.glob(path_4khz+\"4kHz/*\")\n",
    "\n",
    "# remove any previously-processed files\n",
    "out_path = \"/1-fnp/pnwstore1/p-wd05/greenland/resampled/\"\n",
    "out_files = glob.glob(out_path+\"*\")\n",
    "out_files = [f.split(\"/\")[-1] for f in out_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd973f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Read 4khz segy files using obspy, downsample, and write\n",
    "\n",
    "'''\n",
    "\n",
    "# set parameters\n",
    "fs_new = 1000\n",
    "nx_new = 30000\n",
    "nx_old = 120000\n",
    "\n",
    "# get list of files to process\n",
    "arg_list = []\n",
    "for f in files_4khz:\n",
    "    if f.split(\"/\")[-1] not in out_files:\n",
    "        arg_list.append(f)\n",
    "        \n",
    "# run in serial\n",
    "for f in arg_list:\n",
    "    downsample_file(f, out_path, nx_old, nx_new, fs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fde0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the natively 1khz files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "\n",
    "# list all the resamopled 1khz files\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "\n",
    "# combine and sort\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort()\n",
    "\n",
    "# set output path\n",
    "out_path = \"/fd1/solinger/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7db749",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Combine files into mseed files for each channel for the entire dataset\n",
    "\n",
    "'''\n",
    "\n",
    "# choose which channels to write continous files for\n",
    "start_channel = 331\n",
    "end_channel = 1362\n",
    "channels = np.arange(start_channel,end_channel,10)\n",
    "\n",
    "# make empty stream to fill\n",
    "st = obspy.Stream()\n",
    "\n",
    "# iterate through each 30 second file\n",
    "for f in files_1khz_all:\n",
    "    try:\n",
    "        # read in all channels for current file\n",
    "        st_tmp = _read_segy(f)\n",
    "\n",
    "        # add each channel to cumulative stream\n",
    "        for channel in channels:\n",
    "            st_tmp[channel].stats.station = str(channel)\n",
    "            st += st_tmp[channel]\n",
    "    except:\n",
    "        print(\"Issue processing file \" + f)\n",
    "\n",
    "# merge into one stream \n",
    "st.merge(fill_value=0)\n",
    "\n",
    "# write a separate file for each channel\n",
    "for channel in channels:\n",
    "    st_channel = st.select(station=str(channel))\n",
    "    st_channel.write(out_path + \"channel_\"+str(channel)+\".mseed\",format=\"MSEED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cef7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file,channels):\n",
    "    st = _read_segy(file)\n",
    "    t = st[0].times(type=\"utcdatetime\")\n",
    "    data = np.zeros((st[0].stats.npts,len(channels)))\n",
    "    for i,channel in enumerate(channels):\n",
    "        data[:,i] = st[channel].data\n",
    "    return data,t\n",
    "\n",
    "\n",
    "def design_butter(lowcut, highcut, fs, order=4):\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        sos = butter(order, [low, high], analog=False, btype='band', output='sos')\n",
    "        return sos\n",
    "\n",
    "    \n",
    "def filter_butter(data, lowcut, highcut, fs, order=4):\n",
    "        sos = design_butter(lowcut, highcut, fs, order=order)\n",
    "        y = sosfilt(sos, data, axis=0)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "def taper(data,taper_length,fs):\n",
    "    taper_length = int(taper_length*2*fs)\n",
    "    window_ends = hann(taper_length*2)\n",
    "    window = np.ones(data.shape)\n",
    "    for i in range(window.shape[1]):\n",
    "        window[0:taper_length,i] = window_ends[0:taper_length]\n",
    "        window[-taper_length:window.shape[0],i] = window_ends[-taper_length:window.shape[0]]\n",
    "    data = window * data\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data(st,s):\n",
    "    data = np.zeros((s.file_length*s.fs,len(s.channels)))\n",
    "    for i,channel in enumerate(s.channels):\n",
    "        data[:,i] = st.select(station=str(channel))[0].data\n",
    "    return data\n",
    "    \n",
    "\n",
    "def save_data_h5(s):\n",
    "    \n",
    "    # get start and endtime for saving\n",
    "    startstr = s.t[0].datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    endstr = s.t[-1].datetime.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    timestamps = [time.datetime.timestamp() for time in s.t]\n",
    "    \n",
    "    # save result for this file\n",
    "    fname = (s.out_path+\"chans\"+str(s.channels[0])+\"-\"+str(s.channels[-1])+\n",
    "            \"_fs\"+str(int(s.fs_new))+\"_\"+startstr+\"-\"+endstr+\".h5\")\n",
    "    \n",
    "    # save array of data to h5\n",
    "    with h5py.File(fname,\"w\") as f:\n",
    "        f[\"data\"] = s.data\n",
    "        f[\"timestamps\"] = timestamps\n",
    "        f.attrs[\"starttime\"] = s.t[0].datetime.strftime(\"%Y-%m-%dT%H:%M:%S%f\")\n",
    "        f.attrs[\"endtime\"] = s.t[-1].datetime.strftime(\"%Y-%m-%dT%H:%M:%S%f\")\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "def read_and_preprocess(s):\n",
    "    try:\n",
    "        # read the file and preprocess\n",
    "        data,s.t = read_data(s.file,s.channels)\n",
    "        data = preprocess_data(data,s)\n",
    "        s.data = data\n",
    "    except:\n",
    "        print(\"Issue processing file \" + f)\n",
    "        s.data = []\n",
    "        s.t = []\n",
    "        \n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_data(data,s):\n",
    "    data = detrend(data, axis=0, type='linear')\n",
    "    data = taper(data,s.taper_length,s.fs)\n",
    "    sos = design_butter(s.freq[0],s.freq[1],s.fs)\n",
    "    data = filter_butter(data,s.freq[0],s.freq[1],s.fs)\n",
    "    s.fs_new = s.freq[1]*2\n",
    "    data = decimate(data,int(s.fs/s.fs_new),axis=0)\n",
    "    return data            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Save the entire preprocessed dataset as h5 files\n",
    "\n",
    "FIGURE OUT WHY THIS DOESNT EXECUTE PROPERLY WHEN DEFINED IN MODULE!\n",
    "RUNS BUT DOESN'T ERROR WHEN TRYING TO READ BAD FILE WHEN...\n",
    "    1. EVERYTHING IN MODULE WITHOUT __name__ ==__'main'__ \n",
    "    2. FUNCTIONS DEFINED IN MODULE WITH imap_unordered IN __name__ ==__'main'__ IN NOTEBOOK\n",
    "RUNS PROPERLY WHEN EVERYTHING IS DEFINED IN NOTEBOOK WITH imap_unordered IN __name__ ==__'main'__\n",
    "\n",
    "'''\n",
    "\n",
    "# list all the files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort() \n",
    "\n",
    "# set save file saving parameters\n",
    "s = types.SimpleNamespace()\n",
    "s.channels = range(331,1361)\n",
    "s.file_length = 30\n",
    "s.duration = 300\n",
    "s.fs = 1000\n",
    "s.freq = [10,100]\n",
    "s.taper_length = 0.25\n",
    "s.out_path = \"/store_ssd4/greenland/data/\"\n",
    "s.files = files_1khz_all[7340:]\n",
    "s.n_procs = 24\n",
    "\n",
    "# save the dataset as h5 files\n",
    "#save_dataset_h5(s)\n",
    "\n",
    "# read the relevant files, fill metadata, taper, and filter\n",
    "inputs = []\n",
    "for f in s.files:\n",
    "\n",
    "    # add to preprocessing input list\n",
    "    s.file = f\n",
    "    inputs.append(copy.deepcopy(s))\n",
    "\n",
    "# map inputs to read_preprocess_save as each call finishes\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "    p = multiprocessing.Pool(processes=s.n_procs)\n",
    "    for result in p.imap_unordered(read_and_preprocess,inputs):\n",
    "        if len(result.data) != 0:\n",
    "            save_data_h5(result)\n",
    "    p.close()\n",
    "    p.join()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
