{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b3afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy_local\n",
    "import obspy\n",
    "from obspy_local.obspy_local.io.segy.core import _read_segy\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69e2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for converting files\n",
    "def downsample_file(f, out_path, nx_old, nx_new, fs_new):\n",
    "    try:\n",
    "        st = _read_segy(f,npts=nx_old)\n",
    "\n",
    "        # resample\n",
    "        st = st.resample(fs_new)\n",
    "\n",
    "        # correct metadata\n",
    "        st.stats.binary_file_header.number_of_samples_per_data_trace = nx_new\n",
    "        st.stats.binary_file_header.sample_interval_in_microseconds = int(1/fs_new*1e6)\n",
    "\n",
    "        # convert to float32\n",
    "        for tr in st:\n",
    "            tr.data = tr.data.astype('float32')\n",
    "\n",
    "        # write file\n",
    "        st.write(out_path+f.split(\"/\")[-1],format=\"SEGY\")\n",
    "    except:\n",
    "        print(\"Issue processing file \" + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the 4khz Greenland files\n",
    "path_4khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/\"\n",
    "files_4khz = glob.glob(path_4khz+\"4kHz/*\")\n",
    "\n",
    "# remove any previously-processed files\n",
    "out_path = \"/1-fnp/pnwstore1/p-wd05/greenland/resampled/\"\n",
    "out_files = glob.glob(out_path+\"*\")\n",
    "out_files = [f.split(\"/\")[-1] for f in out_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd973f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Read 4khz segy files using obspy, downsample, and write\n",
    "\n",
    "'''\n",
    "\n",
    "# set parameters\n",
    "fs_new = 1000\n",
    "nx_new = 30000\n",
    "nx_old = 120000\n",
    "\n",
    "# get list of files to process\n",
    "arg_list = []\n",
    "for f in files_4khz:\n",
    "    if f.split(\"/\")[-1] not in out_files:\n",
    "        arg_list.append(f)\n",
    "        \n",
    "# run in serial\n",
    "for f in arg_list:\n",
    "    downsample_file(f, out_path, nx_old, nx_new, fs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5fde0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the natively 1khz files\n",
    "path_1khz = \"/1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/*\"\n",
    "path_resampled = \"/1-fnp/pnwstore1/p-wd05/greenland/data/resampled/*\"\n",
    "files_1khz = glob.glob(path_1khz)\n",
    "\n",
    "# list all the resamopled 1khz files\n",
    "files_resampled = glob.glob(path_resampled)\n",
    "\n",
    "# combine and sort\n",
    "files_1khz_all = files_1khz + files_resampled\n",
    "files_1khz_all.sort()\n",
    "\n",
    "# set output path\n",
    "out_path = \"/fd1/solinger/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7db749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue processing file /1-fnp/petasaur/p-wd03/greenland/Store Glacier DAS data/1kHz/Greenland_iDAS15040_ContinuousAQ_190705120337.sgy\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Combine files into mseed files for each channel for the entire dataset\n",
    "\n",
    "'''\n",
    "\n",
    "# choose which channels to write continous files for\n",
    "start_channel = 331\n",
    "end_channel = 1362\n",
    "channels = np.arange(start_channel,end_channel,10)\n",
    "\n",
    "# make empty stream to fill\n",
    "st = obspy.Stream()\n",
    "\n",
    "# iterate through each 30 second file\n",
    "for f in files_1khz_all:\n",
    "    try:\n",
    "        # read in all channels for current file\n",
    "        st_tmp = _read_segy(f)\n",
    "\n",
    "        # add each channel to cumulative stream\n",
    "        for channel in channels:\n",
    "            st_tmp[channel].stats.station = str(channel)\n",
    "            st += st_tmp[channel]\n",
    "    except:\n",
    "        print(\"Issue processing file \" + f)\n",
    "\n",
    "# merge into one stream \n",
    "st.merge(fill_value=0)\n",
    "\n",
    "# write a separate file for each channel\n",
    "for channel in channels:\n",
    "    st_channel = st.select(station=str(channel))\n",
    "    st_channel.write(out_path + \"channel_\"+str(channel)+\".mseed\",format=\"MSEED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
